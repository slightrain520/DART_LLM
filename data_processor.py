"""
æ•°æ®å¤„ç†æ¨¡å— - å¯¹æœç´¢ç»“æœè¿›è¡ŒäºŒæ¬¡æ•´ç†
åŒ…æ‹¬ï¼š
1. extract_context() - æç‚¼ä¸Šä¸‹æ–‡ï¼Œé€‰å–ç›¸å…³æ–‡ä»¶ï¼Œæ§åˆ¶é•¿åº¦
2. files_to_citations() - ç”Ÿæˆå¼•ç”¨ç¼–å·/é“¾æ¥ï¼Œä¾¿äºç­”æ¡ˆå¼•ç”¨
"""
import requests
from typing import List, Dict, Tuple, Any
TOKEN = "e-1qa4tLR9N_AnEEBemwaiOBoyoRoFHr00W0Wb3Uk5tWE5ziWJiCHh7sM1b73T2s"
DBNAME = "student_Group12_final"
METRIC_TYPE = "cosine"
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ï¼Œç”¨äºå¤„ç†RAGæ£€ç´¢ç»“æœ"""
    
    def __init__(self, max_context_length: int = 2000, top_k: int = 10):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
            top_k: é€‰å–çš„æœ€ç›¸å…³æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤ä¸º10
        """
        self.max_context_length = max_context_length
        self.top_k = top_k
    
    def search_knowledge_base(
        self, 
        query: str, 
        base_url: str = "http://10.1.0.220:9002/api",
        db_name: str = "common_dataset",
        token: str = "token_common",
        top_k: int = None,
        metric_type: str = "cosine",
        score_threshold: float = 0.0

    ) -> Dict[str, Any]:
        """
        è°ƒç”¨åç«¯æœç´¢æ¥å£ï¼Œä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            base_url: APIåŸºç¡€URL
            db_name: æ•°æ®åº“åç§°ï¼ˆé»˜è®¤ä½¿ç”¨å…±äº«æ•°æ®åº“ï¼‰
            token: è®¿é—®ä»¤ç‰Œ
            top_k: è¿”å›çš„æœ€ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            metric_type: ç›¸ä¼¼åº¦åº¦é‡ç±»å‹
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼,é»˜è®¤ä¸º0.5
            
        Returns:
            æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        if top_k is None:
            top_k = self.top_k

            
        search_url = f"{base_url}/databases/{db_name}/search"
        payload = {
            "token": token,
            "query": query,
            "top_k": top_k,
            "metric_type": metric_type,
            "score_threshold": score_threshold
        }
        
        try:
            response = requests.post(search_url, json=payload, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"æœç´¢è¯·æ±‚å¤±è´¥: {e}")
            return {"results": []}
    
    def process_context(
        self, 
        search_results: Dict[str, Any],
        max_length: int = None
    ) -> Tuple[str, List[Dict[str, Any]]]:
        """
        ä»æœç´¢ç»“æœä¸­æ•´ç†ä¸Šä¸‹æ–‡ï¼Œé€‰å–ç›¸å…³æ–‡ä»¶å†…å®¹å¹¶æ§åˆ¶é•¿åº¦
        
        Args:
            search_results: åç«¯æœç´¢æ¥å£è¿”å›çš„ç»“æœ
            max_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„é…ç½®ï¼‰
            
        Returns:
            (context_text, filtered_results): 
                - context_text: æ•´ç†åçš„ä¸Šä¸‹æ–‡æ–‡æœ¬,æ™ºèƒ½æˆªæ–­
                - filtered_results: ç­›é€‰å’Œå¤„ç†åçš„ç»“æœåˆ—è¡¨,åŒ…å«å¼•ç”¨çš„æ–‡ä»¶ä¿¡æ¯å’Œå†…å®¹é¢„è§ˆ
        """
        if max_length is None:
            max_length = self.max_context_length# è°ƒç”¨æ—¶æœªæ˜¾ç¤ºæŒ‡å®šåˆ™å¯ç”¨é»˜è®¤å€¼
        
        # è·å–æœç´¢ç»“æœåˆ—è¡¨
        #æ­¤å¤„æœç´¢ç»“æœä¸ºjsonåˆ—è¡¨ï¼Œsearch_results.json()["files"]
        results = search_results.get("files", [])
        
        if not results:
            return "", []
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°æ’åºï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸ä¼¼ï¼‰
        sorted_results = sorted(
            results, 
            key=lambda x: x.get("score", 0), 
            reverse=True
        )
        
        # é€‰å–top_kä¸ªæœ€ç›¸å…³çš„ç»“æœ
        top_results = sorted_results[:self.top_k]
        
        # æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
        context_parts = []
        filtered_results = []
        current_length = 0
        
        for idx, result in enumerate(top_results, 1):
            # æå–æ–‡ä»¶å†…å®¹å’Œå…ƒæ•°æ®
            file_content = result.get("text", "")
            file_id = result.get("file_id", "unknown")
            score = result.get("score", 0)
            metadata = result.get("metadata", {})
            
            # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œåœæ­¢æ·»åŠ 
            if current_length >= max_length:
                break
            
            # è®¡ç®—å¯ç”¨ç©ºé—´
            available_space = max_length - current_length
            
            # æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”å‰©ä½™ç©ºé—´
            if len(file_content) > available_space:
                # ä¿ç•™å®Œæ•´çš„å¥å­ï¼Œé¿å…åœ¨å¥å­ä¸­é—´æˆªæ–­
                truncated_content = self._smart_truncate(file_content, available_space)
            else:
                truncated_content = file_content
            
            # æ„å»ºå¸¦å¼•ç”¨ç¼–å·çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ
            context_piece = f"[æ–‡æ¡£{idx}] (ç›¸ä¼¼åº¦: {score:.4f})\n{truncated_content}\n"
            context_parts.append(context_piece)
            
            # è®°å½•ç­›é€‰åçš„ç»“æœ,List(Dict)
            filtered_results.append({
                "citation_id": idx,
                "file_id": file_id,
                "content": truncated_content,
                "score": score,
                "metadata": metadata
            })
            
            current_length += len(context_piece)
        
        # åˆå¹¶æ‰€æœ‰ä¸Šä¸‹æ–‡ç‰‡æ®µ
        context_text = "\n".join(context_parts)
        
        return context_text, filtered_results
    
    def _smart_truncate(self, text: str, max_length: int) -> str:
        """
        æ™ºèƒ½æˆªæ–­æ–‡æœ¬ï¼Œå°½é‡ä¿æŒå¥å­å®Œæ•´æ€§
        
        Args:
            text: å¾…æˆªæ–­çš„æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        if len(text) <= max_length:
            return text
        
        # åœ¨æœ€å¤§é•¿åº¦é™„è¿‘å¯»æ‰¾å¥å­ç»“æŸæ ‡è®°
        truncated = text[:max_length]
        
        # å¥å­ç»“æŸç¬¦
        sentence_ends = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n']
        
        # ä»åå‘å‰æŸ¥æ‰¾æœ€è¿‘çš„å¥å­ç»“æŸç¬¦
        last_end_pos = -1
        for end_char in sentence_ends:
            pos = truncated.rfind(end_char)
            if pos > last_end_pos:
                last_end_pos = pos
        
        # å¦‚æœæ‰¾åˆ°å¥å­ç»“æŸç¬¦ä¸”ä½ç½®åˆç†ï¼ˆè‡³å°‘ä¿ç•™ä¸€åŠå†…å®¹ï¼‰
        if last_end_pos > max_length // 2:
            return truncated[:last_end_pos + 1]
        
        # å¦åˆ™ç›´æ¥æˆªæ–­å¹¶æ·»åŠ çœç•¥å·
        return truncated + "..."
    
    def files_to_citations(
        self, 
        filtered_results: List[Dict[str, Any]]
    ) -> Dict[int, Dict[str, Any]]:
        """
        ç”Ÿæˆå¼•ç”¨ç¼–å·å’Œæ–‡ä»¶ä¿¡æ¯çš„æ˜ å°„ï¼Œä¾¿äºåœ¨ç­”æ¡ˆä¸­å¼•ç”¨
        
        Args:
            filtered_results: extract_contextè¿”å›çš„ç­›é€‰ç»“æœåˆ—è¡¨
            
        Returns:
            å¼•ç”¨å­—å…¸ï¼Œæ ¼å¼ä¸º {citation_id: {file_id, score, metadata, ...}}
        """
        citations = {}
        
        for result in filtered_results:
            citation_id = result.get("citation_id")
            citations[citation_id] = {
                "file_id": result.get("file_id"),
                "score": result.get("score"),
                "metadata": result.get("metadata", {}),
                "content_preview": result.get("content")[:100] + "..." 
                    if len(result.get("content", "")) > 100 else result.get("content", "")
            }
        
        return citations
    
    def format_citations_for_display(
        self, 
        citations: Dict[int, Dict[str, Any]]
    ) -> str:
        """
        æ ¼å¼åŒ–å¼•ç”¨ä¿¡æ¯ï¼Œç”¨äºæ˜¾ç¤ºç»™ç”¨æˆ·
        
        Args:
            citations: å¼•ç”¨å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–åçš„å¼•ç”¨æ–‡æœ¬
        """
        if not citations:
            return ""
        
        citation_lines = ["\n\nğŸ“š å‚è€ƒæ–‡æ¡£ï¼š"]
        
        for citation_id, info in sorted(citations.items()):
            file_id = info.get("file_id", "unknown")
            score = info.get("score", 0)
            metadata = info.get("metadata", {})
            description = metadata.get("description", "æ— æè¿°")
            
            citation_line = f"[{citation_id}] æ–‡ä»¶ID: {file_id} | ç›¸ä¼¼åº¦: {score:.4f} | {description}"
            citation_lines.append(citation_line)
        
        return "\n".join(citation_lines)


def extract_context(
    query: str,
    base_url: str = "http://10.1.0.220:9002/api",
    db_name: str = "common_dataset",# é»˜è®¤ä½¿ç”¨å…±äº«æ•°æ®åº“
    token: str = "token_common",
    max_context_length: int = 2000,
    top_k: int = 5,
    score_threshold: float = 0.0,
    metric_type: str = "cosine"
) -> Tuple[str, List[Dict[str, Any]], Dict[int, Dict[str, Any]]]:
    """
    è¿™æ˜¯ä¸»è¦çš„å¯¹å¤–æ¥å£å‡½æ•°ï¼Œå®Œæˆå®Œæ•´çš„RAGæ£€ç´¢æµç¨‹ï¼š
    1. è°ƒç”¨åç«¯æœç´¢æ¥å£
    2. æå–å’Œæ•´ç†ä¸Šä¸‹æ–‡
    3. ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        base_url: APIåŸºç¡€URL
        db_name: æ•°æ®åº“åç§°
        token: è®¿é—®ä»¤ç‰Œ
        max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        top_k: è¿”å›çš„æœ€ç›¸ä¼¼æ–‡æ¡£æ•°é‡
        
    Returns:
        (context_text, filtered_results, citations):
            - context_text: æå–çš„ä¸Šä¸‹æ–‡ï¼Œä¼šåˆ—å‡ºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ã€ç›¸ä¼¼åº¦ï¼Œå¹¶æ™ºèƒ½æˆªæ–­ï¼Œå»ºè®®ç»™LLMä½¿ç”¨
            - filtered_results: è¯¦ç»†ç»“æœï¼ŒåŒ…å«å¼•ç”¨IDï¼Œæ–‡ä»¶IDï¼Œç›¸ä¼¼åº¦ï¼Œå†…å®¹é¢„è§ˆï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹æ£€ç´¢å†…å®¹
            - citations: å¼•ç”¨ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ–‡ä»¶IDï¼Œç›¸ä¼¼åº¦å’Œdescriptionï¼ˆç±»ä¼¼å‚è€ƒæ–‡çŒ®åˆ—è¡¨ï¼‰
            
    Example:
        >>> context, results, citations = extract_context(
        ...     query="ä»€ä¹ˆæ˜¯ç½‘ç»œå®‰å…¨ï¼Ÿ",#ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢
        ...     max_context_length=1500,# æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œä»é…ç½®æ–‡ä»¶è¯»å–
        ...     top_k=3# è¿”å›çš„æœ€ç›¸ä¼¼æ–‡æ¡£æ•°é‡
        ... )
        >>> print(context)
        >>> print(citations)
    """
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨,ä½¿ç”¨æŒ‡å®šçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦å’Œtop_k
    processor = DataProcessor(max_context_length=max_context_length, top_k=top_k)
    
    # 1. è°ƒç”¨æœç´¢æ¥å£
    search_results = processor.search_knowledge_base(
        query=query,
        base_url=base_url,
        db_name=db_name,
        token=token,
        top_k=top_k,
        score_threshold=score_threshold,
        metric_type=metric_type
    )
    
    # 2. æå–ä¸Šä¸‹æ–‡
    context_text, filtered_results = processor.process_context(search_results)
    
    # 3. ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
    citations = processor.files_to_citations(filtered_results)
    
    return context_text, filtered_results, citations


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•RAGæ£€ç´¢åŠŸèƒ½
    test_query = "é’“é±¼é‚®ä»¶"
    
    print(f"æ­£åœ¨æ£€ç´¢: {test_query}")
    print("=" * 60)
    
    context, results, citations = extract_context(
        query=test_query,
        max_context_length=1500,
        top_k=5,
        score_threshold=0.2,
        metric_type=METRIC_TYPE,
        db_name = DBNAME,
        token=TOKEN
    )
    
    print("\næå–çš„ä¸Šä¸‹æ–‡ï¼š")
    print("-" * 60)
    print(context)
    
    print("\n\nå¼•ç”¨ä¿¡æ¯ï¼š")
    print("-" * 60)
    processor = DataProcessor()
    print(processor.format_citations_for_display(citations))
    
    print("\n\nè¯¦ç»†ç»“æœï¼š")
    print("-" * 60)
    for result in results:
        print(f"å¼•ç”¨ID: {result['citation_id']}")
        print(f"æ–‡ä»¶ID: {result['file_id']}")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"å†…å®¹é¢„è§ˆ: {result['content']}...")
        print("-" * 40)
