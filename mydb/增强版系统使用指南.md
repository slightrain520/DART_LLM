# å¢å¼ºç‰ˆç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»ºç³»ç»Ÿä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [åŠŸèƒ½æ¨¡å—](#åŠŸèƒ½æ¨¡å—)
4. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
5. [PDFå¤„ç†æ¥å£](#pdfå¤„ç†æ¥å£)
6. [æ•°æ®æºæ¨è](#æ•°æ®æºæ¨è)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç³»ç»Ÿæ¦‚è¿°

### ä¸»è¦ç‰¹æ€§

âœ… **æ”¹è¿›çš„æ–‡æœ¬æ¸…æ´—**
- ç§»é™¤æ— æ„ä¹‰å­—ç¬¦å’Œé‡å¤å†…å®¹
- è¿‡æ»¤å¯¼èˆªæ ã€é¡µè„šç­‰å™ªå£°
- æ™ºèƒ½æå–æœ‰æ„ä¹‰çš„å¥å­

âœ… **æ™ºèƒ½åˆ†å—**
- åŸºäºè¯­ä¹‰è¾¹ç•Œåˆ†å‰²ï¼ˆæ®µè½ã€å¥å­ï¼‰
- è‡ªåŠ¨æ·»åŠ chunké‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
- å»é‡å’Œè´¨é‡è¿‡æ»¤

âœ… **è´¨é‡è¯„ä¼°**
- å¤šç»´åº¦è¯„åˆ†ï¼ˆé•¿åº¦ã€ç›¸å…³æ€§ã€å¯è¯»æ€§ã€ä¿¡æ¯å¯†åº¦ï¼‰
- è‡ªåŠ¨è¿‡æ»¤ä½è´¨é‡å†…å®¹
- å¯è°ƒèŠ‚çš„è´¨é‡é˜ˆå€¼

âœ… **å¤šæ•°æ®æºæ”¯æŒ**
- ç½‘é¡µçˆ¬å–ï¼ˆHTMLï¼‰
- PDFæ–‡ä»¶å¤„ç†ï¼ˆæœ¬åœ°å’Œè¿œç¨‹ï¼‰
- ä¸­è‹±æ–‡å†…å®¹æ”¯æŒ

âœ… **PDFæ™ºèƒ½å¤„ç†**
- æ–‡æœ¬æå–å’Œæ¸…æ´—
- æ™ºèƒ½åˆ†å—å’Œè´¨é‡æ§åˆ¶
- æ‰¹é‡å¤„ç†å’Œä¸Šä¼ æ¥å£

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install requests beautifulsoup4 pdfminer.six
```

### 2. æµ‹è¯•æ¨¡å¼è¿è¡Œ

```bash
cd d:\lhy\DART_LLM\mydb
python enhanced_crawler.py
```

æµ‹è¯•æ¨¡å¼ä¼šçˆ¬å–å°‘é‡é¡µé¢ï¼ˆæ¯ä¸ªç±»åˆ«2ä¸ªURLï¼‰ï¼Œç”¨äºéªŒè¯ç³»ç»ŸåŠŸèƒ½ã€‚

### 3. æŸ¥çœ‹ç»“æœ

è¿è¡Œå®Œæˆåä¼šæ˜¾ç¤ºï¼š
- æ•°æ®åº“åç§°
- å¤„ç†ç»Ÿè®¡ä¿¡æ¯
- æœç´¢æµ‹è¯•ç»“æœ

---

## åŠŸèƒ½æ¨¡å—

### 1. `enhanced_crawler.py` - å¢å¼ºç‰ˆçˆ¬è™«

**ä¸»è¦åŠŸèƒ½ï¼š**
- ç½‘é¡µå’ŒPDFçˆ¬å–
- é›†æˆæ–‡æœ¬æ¸…æ´—å’Œè´¨é‡æ§åˆ¶
- æ”¯æŒæµ‹è¯•æ¨¡å¼å’Œå®Œæ•´æ¨¡å¼

**æ ¸å¿ƒç±»ï¼š**

```python
from mydb.enhanced_crawler import EnhancedCrawler

# åˆå§‹åŒ–çˆ¬è™«
crawler = EnhancedCrawler(
    chunk_size=1500,           # chunkå¤§å°
    chunk_overlap=150,         # é‡å å¤§å°
    min_chunk_size=100,        # æœ€å°chunk
    quality_threshold=0.3,     # è´¨é‡é˜ˆå€¼
    aggressive_cleaning=False  # æ˜¯å¦æ¿€è¿›æ¸…æ´—
)

# å¤„ç†URLåˆ—è¡¨
urls = ["https://example.com/article1", "https://example.com/doc.pdf"]
chunks = crawler.process_urls(urls, delay=1.0)

# æ‰“å°ç»Ÿè®¡
crawler.print_stats()
```

### 2. `pdf_processor.py` - PDFå¤„ç†æ¨¡å—

**ä¸»è¦åŠŸèƒ½ï¼š**
- æœ¬åœ°PDFæ–‡ä»¶å¤„ç†
- æ‰¹é‡å¤„ç†ç›®å½•
- ä¸€ç«™å¼ä¸Šä¼ æ¥å£

**æ ¸å¿ƒç±»ï¼š**

```python
from mydb.pdf_processor import PDFProcessor

# åˆå§‹åŒ–å¤„ç†å™¨
processor = PDFProcessor(
    chunk_size=1500,
    chunk_overlap=150,
    quality_threshold=0.3
)

# å¤„ç†å•ä¸ªPDF
chunks = processor.process_pdf(
    "path/to/document.pdf",
    custom_metadata={'document_type': 'æ³•å¾‹æ³•è§„'}
)

# æ‰¹é‡å¤„ç†ç›®å½•
chunks = processor.process_pdf_directory(
    "path/to/pdf_folder",
    recursive=True
)
```

### 3. `text_cleaner.py` - æ–‡æœ¬æ¸…æ´—

**ä¸»è¦åŠŸèƒ½ï¼š**
- ç§»é™¤å™ªå£°å­—ç¬¦
- å»é™¤å¯¼èˆªå’Œé¡µè„š
- æå–æœ‰æ„ä¹‰çš„å¥å­

### 4. `smart_chunker.py` - æ™ºèƒ½åˆ†å—

**ä¸»è¦åŠŸèƒ½ï¼š**
- è¯­ä¹‰æ„ŸçŸ¥åˆ†å—
- æ·»åŠ chunké‡å 
- å»é‡å¤„ç†

---

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šçˆ¬å–ç½‘é¡µå¹¶æ„å»ºçŸ¥è¯†åº“

```python
from mydb.enhanced_crawler import run_enhanced_crawler_demo

# æµ‹è¯•æ¨¡å¼ï¼ˆå°‘é‡é¡µé¢ï¼‰
db_name, file_ids = run_enhanced_crawler_demo(test_mode=True)

# å®Œæ•´æ¨¡å¼ï¼ˆæ‰€æœ‰é¡µé¢ï¼‰
db_name, file_ids = run_enhanced_crawler_demo(test_mode=False)
```

### ç¤ºä¾‹2ï¼šå¤„ç†æœ¬åœ°PDFæ–‡ä»¶

```python
from mydb.pdf_processor import PDFProcessor
from mydb.createdb_pipeline import create_database

# åˆ›å»ºæ•°æ®åº“
db_name, metric = create_database()

# åˆå§‹åŒ–å¤„ç†å™¨
processor = PDFProcessor(quality_threshold=0.3)

# å¤„ç†å¹¶ä¸Šä¼ å•ä¸ªPDF
file_ids = processor.upload_pdf_to_database(
    db_name=db_name,
    pdf_path="æ³•å¾‹æ³•è§„/ç½‘ç»œå®‰å…¨æ³•.pdf",
    custom_metadata={
        'document_type': 'æ³•å¾‹æ³•è§„',
        'category': 'cybersecurity_law',
        'year': '2017'
    }
)

print(f"ä¸Šä¼ æˆåŠŸ: {len(file_ids)} ä¸ªchunk")
```

### ç¤ºä¾‹3ï¼šæ‰¹é‡å¤„ç†PDFç›®å½•

```python
from mydb.pdf_processor import PDFProcessor
from mydb.createdb_pipeline import create_database

# åˆ›å»ºæ•°æ®åº“
db_name, metric = create_database()

# åˆå§‹åŒ–å¤„ç†å™¨
processor = PDFProcessor(quality_threshold=0.3)

# æ‰¹é‡å¤„ç†ç›®å½•
file_ids = processor.upload_pdf_directory_to_database(
    db_name=db_name,
    directory="æ³•å¾‹æ³•è§„æ–‡ä»¶å¤¹",
    recursive=True,  # é€’å½’å¤„ç†å­ç›®å½•
    custom_metadata={
        'document_type': 'æ³•å¾‹æ³•è§„',
        'source': 'local_files'
    }
)

print(f"æ‰¹é‡ä¸Šä¼ æˆåŠŸ: {len(file_ids)} ä¸ªchunk")
processor.print_stats()
```

### ç¤ºä¾‹4ï¼šè‡ªå®šä¹‰URLåˆ—è¡¨çˆ¬å–

```python
from mydb.enhanced_crawler import EnhancedCrawler
from mydb.createdb_pipeline import create_database, upload_chunks

# åˆ›å»ºæ•°æ®åº“
db_name, metric = create_database()

# åˆå§‹åŒ–çˆ¬è™«
crawler = EnhancedCrawler(quality_threshold=0.3)

# è‡ªå®šä¹‰URLåˆ—è¡¨
custom_urls = [
    "https://www.freebuf.com/articles/web/123456.html",
    "https://www.anquanke.com/post/id/234567",
    "https://example.com/security-report.pdf",
]

# çˆ¬å–
chunks = crawler.process_urls(custom_urls, delay=1.0)

# ä¸Šä¼ 
if chunks:
    file_ids = upload_chunks(db_name, chunks)
    print(f"ä¸Šä¼ æˆåŠŸ: {len(file_ids)} ä¸ªchunk")

# æ‰“å°ç»Ÿè®¡
crawler.print_stats()
```

### ç¤ºä¾‹5ï¼šæœç´¢æµ‹è¯•

```python
import requests
from mydb.createdb_pipeline import BASE_URL, TOKEN

db_name = "student_Group12_1234567890"  # æ›¿æ¢ä¸ºå®é™…æ•°æ®åº“å

# æœç´¢æŸ¥è¯¢
query = "SQLæ³¨å…¥æ”»å‡»åŸç†"

payload = {
    "token": TOKEN,
    "query": query,
    "top_k": 5,
    "metric_type": "L2",
    "score_threshold": 0.0
}

response = requests.post(
    f"{BASE_URL}/databases/{db_name}/search",
    json=payload
)

if response.status_code == 200:
    results = response.json().get("files", [])
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
    
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"  ç›¸ä¼¼åº¦: {result.get('score', 0):.4f}")
        print(f"  è´¨é‡åˆ†æ•°: {result.get('metadata', {}).get('quality_score', 'N/A')}")
        print(f"  æ¥æº: {result.get('metadata', {}).get('source_url', 'N/A')}")
        print(f"  å†…å®¹é¢„è§ˆ: {result.get('text', '')[:100]}...")
```

---

## PDFå¤„ç†æ¥å£

### APIå‚è€ƒ

#### 1. å¤„ç†å•ä¸ªPDFæ–‡ä»¶

```python
processor = PDFProcessor()

chunks = processor.process_pdf(
    pdf_path="path/to/file.pdf",
    custom_metadata={
        'document_type': 'æ³•å¾‹æ³•è§„',
        'category': 'cybersecurity_law'
    }
)
```

**å‚æ•°è¯´æ˜ï¼š**
- `pdf_path`: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `custom_metadata`: è‡ªå®šä¹‰å…ƒæ•°æ®å­—å…¸ï¼ˆå¯é€‰ï¼‰

**è¿”å›å€¼ï¼š**
- chunkåˆ—è¡¨ï¼Œæ¯ä¸ªchunkåŒ…å« `file`ï¼ˆæ–‡æœ¬ï¼‰å’Œ `metadata`ï¼ˆå…ƒæ•°æ®ï¼‰

#### 2. æ‰¹é‡å¤„ç†ç›®å½•

```python
chunks = processor.process_pdf_directory(
    directory="pdf_folder",
    recursive=True,
    custom_metadata={'source': 'local'}
)
```

**å‚æ•°è¯´æ˜ï¼š**
- `directory`: ç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `recursive`: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•ï¼ˆé»˜è®¤Falseï¼‰
- `custom_metadata`: è‡ªå®šä¹‰å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

#### 3. ä¸€ç«™å¼ä¸Šä¼ æ¥å£

```python
file_ids = processor.upload_pdf_to_database(
    db_name="student_Group12_xxx",
    pdf_path="path/to/file.pdf",
    custom_metadata={'type': 'regulation'}
)
```

**å‚æ•°è¯´æ˜ï¼š**
- `db_name`: æ•°æ®åº“åç§°ï¼ˆå¿…éœ€ï¼‰
- `pdf_path`: PDFæ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `custom_metadata`: è‡ªå®šä¹‰å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

**è¿”å›å€¼ï¼š**
- file_idåˆ—è¡¨

#### 4. æ‰¹é‡ä¸Šä¼ ç›®å½•

```python
file_ids = processor.upload_pdf_directory_to_database(
    db_name="student_Group12_xxx",
    directory="pdf_folder",
    recursive=True,
    custom_metadata={'source': 'local'}
)
```

### å…ƒæ•°æ®ç»“æ„

æ¯ä¸ªchunkè‡ªåŠ¨ç”Ÿæˆçš„å…ƒæ•°æ®åŒ…æ‹¬ï¼š

```python
{
    'source_url': 'æ–‡ä»¶è·¯å¾„æˆ–URL',
    'title': 'æ–‡æ¡£æ ‡é¢˜',
    'quality_score': 0.85,          # è´¨é‡åˆ†æ•°
    'chunk_index': 0,                # chunkç´¢å¼•
    'total_chunks': 10,              # æ€»chunkæ•°
    'source_type': 'local_pdf',      # æ¥æºç±»å‹
    'file_path': 'å®Œæ•´æ–‡ä»¶è·¯å¾„',
    'file_name': 'æ–‡ä»¶å.pdf',
    'cves': ['CVE-2021-12345'],      # æå–çš„CVEç¼–å·
    'categories': ['sql_injection'], # è‡ªåŠ¨åˆ†ç±»
    # ... ä»¥åŠè‡ªå®šä¹‰å…ƒæ•°æ®
}
```

---

## æ•°æ®æºæ¨è

### å·²é›†æˆçš„æ•°æ®æº

ç³»ç»Ÿå·²ç»é›†æˆäº†ä»¥ä¸‹é«˜è´¨é‡æ•°æ®æºï¼š

#### 1. å›½é™…æƒå¨æ ‡å‡†
- âœ… OWASP Top 10ï¼ˆWebåº”ç”¨å®‰å…¨ï¼‰
- âœ… MITRE ATT&CKï¼ˆæ”»å‡»æŠ€æœ¯ï¼‰
- âœ… CWE Top 25ï¼ˆè½¯ä»¶å¼±ç‚¹ï¼‰
- âœ… PortSwigger Web Security Academy

#### 2. ä¸­æ–‡ç»´åŸºç™¾ç§‘
- âœ… SQLæ³¨å…¥
- âœ… è·¨ç«™è„šæœ¬ï¼ˆXSSï¼‰
- âœ… è·¨ç«™è¯·æ±‚ä¼ªé€ ï¼ˆCSRFï¼‰
- âœ… æ‹’ç»æœåŠ¡æ”»å‡»ï¼ˆDDoSï¼‰
- âœ… é’“é±¼æ”»å‡»
- âœ… å‹’ç´¢è½¯ä»¶
- âœ… é˜²ç«å¢™
- âœ… å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ
- âœ… åŠ å¯†æŠ€æœ¯
- âœ… æ¸—é€æµ‹è¯•

#### 3. è‹±æ–‡ç»´åŸºç™¾ç§‘
- âœ… ç½‘ç»œå®‰å…¨æ ¸å¿ƒæ¦‚å¿µ
- âœ… å¸¸è§æ”»å‡»æŠ€æœ¯
- âœ… é˜²å¾¡æªæ–½

#### 4. æ”¿åºœå’Œæœºæ„èµ„æº
- âœ… NISTç½‘ç»œå®‰å…¨èµ„æº
- âœ… US-CERTå®‰å…¨å…¬å‘Š

### æ¨èæ·»åŠ çš„ä¸­æ–‡æ•°æ®æº

#### 1. æŠ€æœ¯åšå®¢å’Œç¤¾åŒº

**FreeBufï¼ˆå›½å†…çŸ¥åå®‰å…¨ç¤¾åŒºï¼‰**
```python
freebuf_urls = [
    "https://www.freebuf.com/articles/web/[æ–‡ç« ID].html",
    # éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
]
```

**å®‰å…¨å®¢**
```python
anquanke_urls = [
    "https://www.anquanke.com/post/id/[æ–‡ç« ID]",
    # éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
]
```

**å…ˆçŸ¥ç¤¾åŒºï¼ˆé˜¿é‡Œäº‘ï¼‰**
```python
xianzhi_urls = [
    "https://xz.aliyun.com/t/[æ–‡ç« ID]",
    # éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
]
```

#### 2. å‚å•†å®‰å…¨å…¬å‘Š

**å¥‡å®‰ä¿¡å¨èƒæƒ…æŠ¥ä¸­å¿ƒ**
```python
qianxin_urls = [
    "https://ti.qianxin.com/blog/articles/[æ–‡ç« ID]",
]
```

**360å®‰å…¨ä¸­å¿ƒ**
```python
qihoo_urls = [
    "https://www.360.cn/n/[å…¬å‘ŠID].html",
]
```

#### 3. å­¦æœ¯å’Œç ”ç©¶æœºæ„

**ä¸­å›½ä¿¡æ¯å®‰å…¨æµ‹è¯„ä¸­å¿ƒ**
```python
itsec_urls = [
    "http://www.itsec.gov.cn/",
]
```

**å›½å®¶äº’è”ç½‘åº”æ€¥ä¸­å¿ƒï¼ˆCNCERTï¼‰**
```python
cncert_urls = [
    "https://www.cert.org.cn/",
]
```

#### 4. æ³•å¾‹æ³•è§„ï¼ˆPDFæ ¼å¼ï¼‰

æ¨èæ”¶é›†çš„æ³•å¾‹æ³•è§„æ–‡ä»¶ï¼š
- ã€Šä¸­åäººæ°‘å…±å’Œå›½ç½‘ç»œå®‰å…¨æ³•ã€‹
- ã€Šæ•°æ®å®‰å…¨æ³•ã€‹
- ã€Šä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‹
- ã€Šå…³é”®ä¿¡æ¯åŸºç¡€è®¾æ–½å®‰å…¨ä¿æŠ¤æ¡ä¾‹ã€‹
- ã€Šç½‘ç»œå®‰å…¨ç­‰çº§ä¿æŠ¤æ¡ä¾‹ã€‹
- ç­‰ä¿2.0ç›¸å…³æ ‡å‡†æ–‡æ¡£

**ä½¿ç”¨æ–¹æ³•ï¼š**
```python
# å°†PDFæ–‡ä»¶æ”¾å…¥æŒ‡å®šç›®å½•
pdf_dir = "d:/lhy/DART_LLM/mydb/pdf_documents"

# æ‰¹é‡å¤„ç†
processor = PDFProcessor()
file_ids = processor.upload_pdf_directory_to_database(
    db_name=db_name,
    directory=pdf_dir,
    custom_metadata={'document_type': 'æ³•å¾‹æ³•è§„'}
)
```

### æ¨èçš„å›½é™…æ•°æ®æº

#### 1. å®‰å…¨ç ”ç©¶æŠ¥å‘Šï¼ˆPDFï¼‰

**Verizonæ•°æ®æ³„éœ²è°ƒæŸ¥æŠ¥å‘Šï¼ˆDBIRï¼‰**
```python
verizon_urls = [
    "https://www.verizon.com/business/resources/reports/dbir/",
]
```

**SANSç ”ç©¶æ‰€**
```python
sans_urls = [
    "https://www.sans.org/white-papers/",
]
```

#### 2. æ¼æ´æ•°æ®åº“

**NVDï¼ˆå›½å®¶æ¼æ´æ•°æ®åº“ï¼‰**
```python
nvd_urls = [
    "https://nvd.nist.gov/vuln/detail/CVE-2021-44228",  # Log4jç¤ºä¾‹
]
```

**Exploit-DB**
```python
exploitdb_urls = [
    "https://www.exploit-db.com/",
]
```

---

## å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•è°ƒæ•´è´¨é‡é˜ˆå€¼ï¼Ÿ

è´¨é‡é˜ˆå€¼æ§åˆ¶å†…å®¹çš„è¿‡æ»¤ä¸¥æ ¼ç¨‹åº¦ï¼š
- `0.3`ï¼šå®½æ¾ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰
- `0.4`ï¼šä¸­ç­‰ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰
- `0.5`ï¼šä¸¥æ ¼ï¼ˆåªä¿ç•™é«˜è´¨é‡å†…å®¹ï¼‰

```python
crawler = EnhancedCrawler(quality_threshold=0.4)
```

### Q2: å¦‚ä½•å¤„ç†çˆ¬å–å¤±è´¥çš„URLï¼Ÿ

ç³»ç»Ÿä¼šè‡ªåŠ¨è®°å½•å¤±è´¥çš„URLï¼ŒæŸ¥çœ‹æ—¥å¿—ï¼š
```python
crawler.print_stats()
```

å¯ä»¥å•ç‹¬é‡è¯•å¤±è´¥çš„URLï¼š
```python
failed_urls = ["https://example.com/failed"]
chunks = crawler.process_urls(failed_urls)
```

### Q3: PDFæå–å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

å¯èƒ½çš„åŸå› ï¼š
1. PDFåŠ å¯†æˆ–æœ‰å¯†ç ä¿æŠ¤
2. PDFæ˜¯æ‰«æç‰ˆï¼ˆå›¾ç‰‡ï¼‰ï¼Œéœ€è¦OCR
3. PDFæ ¼å¼æŸå

è§£å†³æ–¹æ³•ï¼š
- ä½¿ç”¨PDFè§£å¯†å·¥å…·
- ä½¿ç”¨OCRå·¥å…·ï¼ˆå¦‚Tesseractï¼‰
- ä¿®å¤æˆ–é‡æ–°ä¸‹è½½PDF

### Q4: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰æ•°æ®æºï¼Ÿ

```python
# æ–¹æ³•1ï¼šç›´æ¥æ·»åŠ URL
custom_urls = [
    "https://your-site.com/article1",
    "https://your-site.com/doc.pdf",
]
chunks = crawler.process_urls(custom_urls)

# æ–¹æ³•2ï¼šä¿®æ”¹ enhanced_crawler.py ä¸­çš„ get_cybersecurity_urls()
```

### Q5: å¦‚ä½•æœç´¢ç‰¹å®šç±»å‹çš„æ–‡æ¡£ï¼Ÿ

ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤ï¼ˆéœ€è¦åç«¯APIæ”¯æŒï¼‰ï¼š
```python
payload = {
    "token": TOKEN,
    "query": "SQLæ³¨å…¥",
    "top_k": 5,
    "filter": {
        "document_type": "æ³•å¾‹æ³•è§„"
    }
}
```

### Q6: chunkå¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ

å»ºè®®æ ¹æ®embeddingæ¨¡å‹é€‰æ‹©ï¼š
- **OpenAI text-embedding-ada-002**: 1500-2000å­—ç¬¦
- **BERTç±»æ¨¡å‹**: 500-1000å­—ç¬¦
- **é•¿æ–‡æœ¬æ¨¡å‹**: 2000-4000å­—ç¬¦

```python
crawler = EnhancedCrawler(
    chunk_size=1500,      # æ ¹æ®æ¨¡å‹è°ƒæ•´
    chunk_overlap=150     # é€šå¸¸ä¸ºchunk_sizeçš„10%
)
```

### Q7: å¦‚ä½•æŸ¥çœ‹æ•°æ®åº“ä¸­çš„å†…å®¹ï¼Ÿ

```python
import requests

# åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
response = requests.get(
    f"{BASE_URL}/databases/{db_name}/files",
    params={"token": TOKEN}
)

files = response.json().get("files", [])
print(f"æ•°æ®åº“ä¸­å…±æœ‰ {len(files)} ä¸ªæ–‡ä»¶")
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹é‡å¤„ç†

ä½¿ç”¨æ‰¹é‡æ¥å£è€Œä¸æ˜¯å¾ªç¯å•ä¸ªå¤„ç†ï¼š
```python
# âœ… æ¨è
chunks = crawler.process_urls(url_list)

# âŒ ä¸æ¨è
for url in url_list:
    chunk = crawler.process_url(url)
```

### 2. è°ƒæ•´è¯·æ±‚å»¶è¿Ÿ

æ ¹æ®ç›®æ ‡ç½‘ç«™çš„æ‰¿å—èƒ½åŠ›è°ƒæ•´ï¼š
```python
# å¿«é€Ÿçˆ¬å–ï¼ˆå¯èƒ½è¢«å°ï¼‰
chunks = crawler.process_urls(urls, delay=0.5)

# ç¤¼è²Œçˆ¬å–ï¼ˆæ¨èï¼‰
chunks = crawler.process_urls(urls, delay=1.0)

# ä¿å®ˆçˆ¬å–
chunks = crawler.process_urls(urls, delay=2.0)
```

### 3. ä½¿ç”¨æµ‹è¯•æ¨¡å¼

å¼€å‘æ—¶ä½¿ç”¨æµ‹è¯•æ¨¡å¼ï¼Œé¿å…æµªè´¹æ—¶é—´ï¼š
```python
# æµ‹è¯•æ¨¡å¼
db_name, file_ids = run_enhanced_crawler_demo(test_mode=True)

# ç¡®è®¤æ— è¯¯åä½¿ç”¨å®Œæ•´æ¨¡å¼
db_name, file_ids = run_enhanced_crawler_demo(test_mode=False)
```

---

## ä¸‹ä¸€æ­¥è®¡åˆ’

### çŸ­æœŸæ”¹è¿›
- [ ] æ·»åŠ OCRæ”¯æŒï¼ˆå¤„ç†æ‰«æç‰ˆPDFï¼‰
- [ ] æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼ï¼ˆWordã€Excelï¼‰
- [ ] æ·»åŠ å¹¶å‘çˆ¬å–ï¼ˆæé«˜é€Ÿåº¦ï¼‰
- [ ] æ”¹è¿›ä¸­æ–‡åˆ†è¯å’Œåˆ†å—

### é•¿æœŸè§„åˆ’
- [ ] é›†æˆæ›´å¤šä¸­æ–‡å®‰å…¨ç¤¾åŒº
- [ ] è‡ªåŠ¨åŒ–æ•°æ®æºæ›´æ–°
- [ ] å¢é‡æ›´æ–°æœºåˆ¶
- [ ] æ•°æ®å»é‡å’Œç‰ˆæœ¬ç®¡ç†

---

## æŠ€æœ¯æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
1. æ—¥å¿—è¾“å‡ºï¼ˆåŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼‰
2. `æ”¹è¿›æ–¹æ¡ˆæ€»ç»“.md`ï¼ˆè®¾è®¡æ€è·¯ï¼‰
3. `Pipelineä½¿ç”¨æŒ‡å—.md`ï¼ˆåŸºç¡€ç”¨æ³•ï¼‰

---

**æœ€åæ›´æ–°**: 2025-11-01
**ç‰ˆæœ¬**: v2.0

