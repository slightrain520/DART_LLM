"""
åŸºäºçŸ¥è¯†å›¾è°±çš„é—®ç­”æ•°æ®é›†å¤„ç†å™¨
ä»JSONæ ¼å¼çš„QAæ•°æ®é›†ä¸­æå–é«˜è´¨é‡é—®ç­”å¯¹ï¼Œä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“
"""

import sys
import os
import json
import logging
from typing import List, Dict, Optional

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mydb.createdb_pipeline import (
    BASE_URL, TOKEN, METRIC_TYPE,
    create_database, upload_chunks
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)


class QADatasetProcessor:
    """é—®ç­”æ•°æ®é›†å¤„ç†å™¨"""
    
    def __init__(self, include_question_in_text: bool = True):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            include_question_in_text: æ˜¯å¦åœ¨æ–‡æœ¬ä¸­åŒ…å«é—®é¢˜ï¼ˆæœ‰åŠ©äºè¯­ä¹‰æ£€ç´¢ï¼‰
        """
        self.include_question_in_text = include_question_in_text
        self.stats = {
            'total_qa_pairs': 0,
            'processed_qa_pairs': 0,
            'skipped_qa_pairs': 0,
            'by_method': {},
            'by_file': {}
        }
    
    def load_json_file(self, file_path: str) -> List[Dict]:
        """
        åŠ è½½JSONæ–‡ä»¶
        
        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            QAå¯¹è±¡åˆ—è¡¨
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                logging.warning(f"æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®: {file_path}")
                return []
            
            logging.info(f"âœ“ åŠ è½½æˆåŠŸ: {file_path} | åŒ…å« {len(data)} ä¸ªQAå¯¹")
            return data
            
        except Exception as e:
            logging.error(f"åŠ è½½å¤±è´¥ {file_path}: {e}")
            return []
    
    def process_qa_item(self, qa_item: Dict, source_file: str) -> Optional[Dict]:
        """
        å¤„ç†å•ä¸ªQAé¡¹
        
        Args:
            qa_item: QAæ•°æ®é¡¹
            source_file: æºæ–‡ä»¶å
            
        Returns:
            å¤„ç†åçš„ä¸Šä¼ é¡¹ï¼Œå¦‚æœæ— æ•ˆåˆ™è¿”å›None
        """
        try:
            # æå–å¿…è¦å­—æ®µ
            qid = qa_item.get('QID', '')
            question = qa_item.get('Question', '').strip()
            answer = qa_item.get('Answer', '').strip()
            method = qa_item.get('Method', '')
            entities = qa_item.get('Entity', [])
            relations = qa_item.get('Relation', [])
            ontology = qa_item.get('Ontology', [])
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not question or not answer:
                logging.debug(f"è·³è¿‡æ— æ•ˆQAå¯¹: {qid}")
                self.stats['skipped_qa_pairs'] += 1
                return None
            
            # æ„å»ºæ–‡æœ¬å†…å®¹
            if self.include_question_in_text:
                # æ–¹å¼1: Question + Answerï¼ˆæ›´å¥½çš„è¯­ä¹‰æ£€ç´¢ï¼‰
                text_content = f"é—®é¢˜ï¼š{question}\n\nç­”æ¡ˆï¼š{answer}"
            else:
                # æ–¹å¼2: ä»…Answer
                text_content = answer
            
            # ç»Ÿè®¡æ–¹æ³•ç±»å‹
            if method:
                self.stats['by_method'][method] = self.stats['by_method'].get(method, 0) + 1
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                'qid': qid,
                'question': question,
                'answer': answer,
                'method': method,
                'entities': ','.join(entities) if entities else '',
                'relations': ','.join(relations) if relations else '',
                'source_file': source_file,
                'data_type': 'qa_pair',
                'source': 'AISECKG-QA-Dataset',
                'language': 'en',  # å½“å‰æ•°æ®é›†ä¸ºè‹±æ–‡
            }
            
            # æ·»åŠ æœ¬ä½“ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if ontology:
                # å°†æœ¬ä½“ä¸‰å…ƒç»„è½¬æ¢ä¸ºå¯è¯»å­—ç¬¦ä¸²
                ontology_str = '; '.join([f"{t[0]}-{t[1]}-{t[2]}" for t in ontology if len(t) == 3])
                metadata['ontology'] = ontology_str
            
            self.stats['processed_qa_pairs'] += 1
            
            return {
                'file': text_content,
                'metadata': metadata
            }
            
        except Exception as e:
            logging.error(f"å¤„ç†QAé¡¹å¤±è´¥: {e}")
            self.stats['skipped_qa_pairs'] += 1
            return None
    
    def process_json_file(self, file_path: str) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªJSONæ–‡ä»¶
        
        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†åçš„ä¸Šä¼ é¡¹åˆ—è¡¨
        """
        # åŠ è½½æ•°æ®
        qa_list = self.load_json_file(file_path)
        if not qa_list:
            return []
        
        # è·å–æ–‡ä»¶å
        file_name = os.path.basename(file_path)
        self.stats['by_file'][file_name] = len(qa_list)
        self.stats['total_qa_pairs'] += len(qa_list)
        
        # å¤„ç†æ¯ä¸ªQAå¯¹
        upload_items = []
        for qa_item in qa_list:
            item = self.process_qa_item(qa_item, file_name)
            if item:
                upload_items.append(item)
        
        logging.info(f"âœ“ æ–‡ä»¶å¤„ç†å®Œæˆ: {file_name} | æœ‰æ•ˆQAå¯¹: {len(upload_items)}/{len(qa_list)}")
        
        return upload_items
    
    def process_directory(self, directory: str, file_pattern: str = "*.json") -> List[Dict]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
        
        Args:
            directory: ç›®å½•è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            æ‰€æœ‰å¤„ç†åçš„ä¸Šä¼ é¡¹åˆ—è¡¨
        """
        all_items = []
        
        if not os.path.exists(directory):
            logging.error(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
            return all_items
        
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = [f for f in os.listdir(directory) if f.endswith('.json')]
        
        if not json_files:
            logging.warning(f"ç›®å½•ä¸­æ²¡æœ‰JSONæ–‡ä»¶: {directory}")
            return all_items
        
        logging.info(f"\næ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶:")
        for f in json_files:
            logging.info(f"  - {f}")
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, json_file in enumerate(json_files, 1):
            logging.info(f"\nã€{i}/{len(json_files)}ã€‘å¤„ç†æ–‡ä»¶: {json_file}")
            file_path = os.path.join(directory, json_file)
            items = self.process_json_file(file_path)
            all_items.extend(items)
        
        return all_items
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logging.info("\n" + "=" * 80)
        logging.info("QAæ•°æ®é›†å¤„ç†ç»Ÿè®¡")
        logging.info("=" * 80)
        logging.info(f"æ€»QAå¯¹æ•°: {self.stats['total_qa_pairs']}")
        logging.info(f"æˆåŠŸå¤„ç†: {self.stats['processed_qa_pairs']}")
        logging.info(f"è·³è¿‡æ•°é‡: {self.stats['skipped_qa_pairs']}")
        
        if self.stats['by_file']:
            logging.info("\næŒ‰æ–‡ä»¶ç»Ÿè®¡:")
            for file_name, count in self.stats['by_file'].items():
                logging.info(f"  - {file_name}: {count} å¯¹")
        
        if self.stats['by_method']:
            logging.info("\næŒ‰æ–¹æ³•ç»Ÿè®¡:")
            for method, count in self.stats['by_method'].items():
                logging.info(f"  - {method}: {count} å¯¹")
        
        if self.stats['total_qa_pairs'] > 0:
            success_rate = self.stats['processed_qa_pairs'] / self.stats['total_qa_pairs'] * 100
            logging.info(f"\nå¤„ç†æˆåŠŸç‡: {success_rate:.1f}%")
        
        logging.info("=" * 80)


def run_qa_dataset_upload(
    data_directory: str,
    db_name: str = "student_Group12_qa_final",
    metric: str = "cosine",
    include_question: bool = True
):
    """
    è¿è¡ŒQAæ•°æ®é›†ä¸Šä¼ æµç¨‹
    
    Args:
        data_directory: QAæ•°æ®é›†ç›®å½•
        db_name: æ•°æ®åº“åç§°
        metric: ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼
        include_question: æ˜¯å¦åœ¨æ–‡æœ¬ä¸­åŒ…å«é—®é¢˜
    """
    logging.info("=" * 80)
    logging.info("åŸºäºçŸ¥è¯†å›¾è°±çš„QAæ•°æ®é›†ä¸Šä¼ ç³»ç»Ÿ")
    logging.info("=" * 80)
    
    # ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–å¤„ç†å™¨
    logging.info("\nã€æ­¥éª¤1ã€‘åˆå§‹åŒ–QAæ•°æ®é›†å¤„ç†å™¨")
    processor = QADatasetProcessor(include_question_in_text=include_question)
    logging.info(f"âœ“ å¤„ç†å™¨è®¾ç½®: æ–‡æœ¬{'åŒ…å«'if include_question else 'ä¸åŒ…å«'}é—®é¢˜éƒ¨åˆ†")
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ•°æ®é›†
    logging.info("\nã€æ­¥éª¤2ã€‘å¤„ç†QAæ•°æ®é›†æ–‡ä»¶")
    logging.info(f"æ•°æ®ç›®å½•: {data_directory}")
    
    all_items = processor.process_directory(data_directory)
    
    if not all_items:
        logging.error("âŒ æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æœ‰æ•ˆçš„QAå¯¹ï¼")
        return None, []
    
    # ç¬¬ä¸‰æ­¥ï¼šæ‰“å°ç»Ÿè®¡
    processor.print_stats()
    
    # ç¬¬å››æ­¥ï¼šåˆ›å»º/ä½¿ç”¨æ•°æ®åº“
    logging.info("\nã€æ­¥éª¤3ã€‘å‡†å¤‡å‘é‡æ•°æ®åº“")
    logging.info(f"æ•°æ®åº“åç§°: {db_name}")
    logging.info(f"ç›¸ä¼¼åº¦åº¦é‡: {metric}")
    
    # ç¬¬äº”æ­¥ï¼šä¸Šä¼ æ•°æ®
    logging.info("\nã€æ­¥éª¤4ã€‘ä¸Šä¼ QAæ•°æ®åˆ°å‘é‡æ•°æ®åº“")
    logging.info(f"å‡†å¤‡ä¸Šä¼  {len(all_items)} ä¸ªQAå¯¹...")
    
    try:
        file_ids = upload_chunks(db_name, all_items)
        logging.info(f"âœ“ ä¸Šä¼ å®Œæˆï¼å…±ä¸Šä¼  {len(file_ids)} ä¸ªQAå¯¹")
    except Exception as e:
        logging.error(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        return None, []
    
    # ç¬¬å…­æ­¥ï¼šæ€»ç»“
    logging.info("\n" + "=" * 80)
    logging.info("ğŸ‰ QAæ•°æ®é›†ä¸Šä¼ å®Œæˆï¼")
    logging.info("=" * 80)
    logging.info(f"ğŸ“Š æ•°æ®åº“åç§°: {db_name}")
    logging.info(f"ğŸ“Š ä¸Šä¼ QAå¯¹æ•°: {len(file_ids)}")
    logging.info(f"ğŸ“Š æ•°æ®æ¥æº: AISECKG-QA-Dataset")
    logging.info("=" * 80)
    
    return db_name, file_ids


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_directory = os.path.join(current_dir, "All_data_files")
    
    # æ•°æ®åº“åç§°ï¼ˆä½¿ç”¨æœ€ç»ˆç‰ˆæœ¬æ•°æ®åº“ï¼‰
    db_name = "student_Group12_final"
    
    # ç›¸ä¼¼åº¦åº¦é‡ï¼ˆä¸åŸæ•°æ®åº“ä¿æŒä¸€è‡´ï¼‰
    metric = "cosine"
    
    # æ˜¯å¦åœ¨æ–‡æœ¬ä¸­åŒ…å«é—®é¢˜ï¼ˆå»ºè®®ä¸ºTrueï¼Œæœ‰åŠ©äºè¯­ä¹‰æ£€ç´¢ï¼‰
    include_question = True
    
    logging.info("é…ç½®ä¿¡æ¯:")
    logging.info(f"  æ•°æ®ç›®å½•: {data_directory}")
    logging.info(f"  æ•°æ®åº“å: {db_name}")
    logging.info(f"  ç›¸ä¼¼åº¦åº¦é‡: {metric}")
    logging.info(f"  åŒ…å«é—®é¢˜: {include_question}")
    
    # è¿è¡Œä¸Šä¼ æµç¨‹
    db_name_result, file_ids = run_qa_dataset_upload(
        data_directory=data_directory,
        db_name=db_name,
        metric=metric,
        include_question=include_question
    )
    
    if db_name_result:
        print("\n" + "=" * 80)
        print("âœ… QAæ•°æ®é›†ä¸Šä¼ æˆåŠŸï¼")
        print(f"æ•°æ®åº“åç§°: {db_name_result}")
        print(f"ä¸Šä¼ QAå¯¹æ•°: {len(file_ids)}")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"1. åœ¨RAGç³»ç»Ÿä¸­ä½¿ç”¨æ•°æ®åº“: '{db_name_result}'")
        print("2. è¿™äº›QAå¯¹åŸºäºçŸ¥è¯†å›¾è°±ç”Ÿæˆï¼Œè´¨é‡è¾ƒé«˜")
        print("3. åŒ…å«ä¸‰ç§æ–¹æ³•: Zero-shot, Ontology-based, In-Context Learning")
        print("=" * 80)
    else:
        print("\nâŒ QAæ•°æ®é›†ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logging.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logging.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise

