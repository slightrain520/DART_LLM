# ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»º Pipeline ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨ `createdb_pipeline.py` æ„å»ºç½‘ç»œå®‰å…¨æ”»é˜²çŸ¥è¯†åº“ã€‚

---

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
4. [æ ¸å¿ƒåŠŸèƒ½ä»‹ç»](#æ ¸å¿ƒåŠŸèƒ½ä»‹ç»)
5. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
6. [è¿›é˜¶ä½¿ç”¨](#è¿›é˜¶ä½¿ç”¨)

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. Pythonç¯å¢ƒè¦æ±‚
- Python 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬
- æ¨èä½¿ç”¨ Python 3.8+

### 2. å®‰è£…ä¾èµ–åº“

```bash
# æ–¹å¼1: ä½¿ç”¨ requirements.txt (å¦‚æœæœ‰)
pip install -r requirements.txt

# æ–¹å¼2: æ‰‹åŠ¨å®‰è£…æ ¸å¿ƒä¾èµ–
pip install requests beautifulsoup4 pdfminer.six
```

**ä¾èµ–åº“è¯´æ˜**:
- `requests`: HTTPè¯·æ±‚åº“ï¼Œç”¨äºä¸‹è½½ç½‘é¡µå’Œæ–‡ä»¶
- `beautifulsoup4`: HTMLè§£æåº“ï¼Œç”¨äºæå–ç½‘é¡µå†…å®¹
- `pdfminer.six`: PDFæ–‡æœ¬æå–åº“ï¼Œç”¨äºå¤„ç†PDFæ–‡æ¡£

### 3. ç½‘ç»œè¿æ¥
- ç¡®ä¿å¯ä»¥è®¿é—®äº’è”ç½‘
- å¦‚æœåœ¨å…¬å¸ç½‘ç»œï¼Œå¯èƒ½éœ€è¦é…ç½®ä»£ç†

### 4. APIé…ç½®
- è·å–åç«¯APIçš„è®¿é—®Token
- ç¡®è®¤åç«¯æœåŠ¡å™¨åœ°å€ï¼ˆBASE_URLï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šé…ç½®å‚æ•°

æ‰“å¼€ `createdb_pipeline.py`ï¼Œä¿®æ”¹é…ç½®åŒºï¼š

```python
# ==================== é…ç½®åŒº ====================
BASE_URL = "http://10.1.0.220:9002/api"   # ä¿®æ”¹ä¸ºä½ çš„åç«¯åœ°å€
TOKEN = "ä½ çš„Token"                        # æ›¿æ¢ä¸ºä½ çš„è®¤è¯Token
USER_NAME = "Group12"                     # æ›¿æ¢ä¸ºä½ çš„ç»„å
METRIC_TYPE = "L2"                        # å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼
```

### ç¬¬äºŒæ­¥ï¼šè¿è¡Œç¨‹åº

```bash
python createdb_pipeline.py
```

### ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹ç»“æœ

ç¨‹åºä¼šè¾“å‡ºå¦‚ä¸‹ä¿¡æ¯ï¼š
```
============================================================
å¼€å§‹ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»ºPipeline
============================================================

ã€æ­¥éª¤1ã€‘åˆ›å»ºå‘é‡æ•°æ®åº“
âœ“ æ•°æ®åº“åˆ›å»ºæˆåŠŸ: student_Group12_1730123456 (metric=L2)

ã€æ­¥éª¤2ã€‘å¼€å§‹çˆ¬å–æ•°æ®æº
>>> æ•°æ®æº1: MITRE ATT&CK æ”»å‡»æŠ€æœ¯åº“
âœ“ æŠ“å–åˆ° 30 ä¸ªMITREé¡µé¢

>>> æ•°æ®æº2: OWASP Top 10 Webå®‰å…¨é£é™©
âœ“ æŠ“å–åˆ° 11 ä¸ªOWASPé¡µé¢

ã€æ­¥éª¤3ã€‘æ–‡æœ¬åˆ†å‰²ä¸å…ƒæ•°æ®ç”Ÿæˆ
âœ“ ç”Ÿæˆ 150 ä¸ªæ–‡æœ¬chunk

ã€æ­¥éª¤4ã€‘æ‰¹é‡ä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“
âœ“ ä¸Šä¼ å®Œæˆï¼Œå…± 150 ä¸ªæ–‡æœ¬å—

ã€æ­¥éª¤5ã€‘æµ‹è¯•æœç´¢åŠŸèƒ½
æµ‹è¯•æŸ¥è¯¢: 'phishing mail indicators'
  [1] ç›¸ä¼¼åº¦: 0.3254
      æ ‡é¢˜: T1566 - Phishing
      åˆ†ç±»: ['phishing', 'attack_technique']

ğŸ‰ Pipelineæ‰§è¡Œå®Œæˆï¼
ğŸ“Š æ•°æ®åº“åç§°: student_Group12_1730123456
ğŸ“Š ä¸Šä¼ æ–‡ä»¶æ•°: 150
ğŸ’¡ åç»­å¯ä»¥ä½¿ç”¨æ­¤æ•°æ®åº“è¿›è¡ŒRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### æ ¸å¿ƒé…ç½®å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | æ¨èå€¼ |
|------|------|--------|--------|
| `BASE_URL` | åç«¯APIåœ°å€ | `http://10.1.0.220:9002/api` | æ ¹æ®å®é™…æƒ…å†µ |
| `TOKEN` | è®¤è¯ä»¤ç‰Œ | éœ€è¦è®¾ç½® | ä»ç®¡ç†å‘˜è·å– |
| `USER_NAME` | ç”¨æˆ·/ç»„å | `Group12` | ä½ çš„ç»„å |
| `METRIC_TYPE` | å‘é‡è·ç¦»åº¦é‡ | `L2` | `L2` æˆ– `cosine` |
| `UPLOAD_BATCH` | æ‰¹é‡ä¸Šä¼ å¤§å° | `10` | 10-50 |
| `PDF_TMP_DIR` | PDFä¸´æ—¶ç›®å½• | ç³»ç»Ÿä¸´æ—¶ç›®å½• | é»˜è®¤å³å¯ |

### METRIC_TYPE è¯´æ˜

- **L2**: æ¬§æ°è·ç¦»ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
- **cosine**: ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé€‚åˆæ–‡æœ¬é•¿åº¦å·®å¼‚å¤§çš„åœºæ™¯
- **IP**: å†…ç§¯ï¼Œè¾ƒå°‘ä½¿ç”¨

---

## ğŸ§© æ ¸å¿ƒåŠŸèƒ½ä»‹ç»

### 1. ç½‘é¡µçˆ¬å– (`safe_request_get`)

**åŠŸèƒ½**: å®‰å…¨åœ°å‘é€HTTP GETè¯·æ±‚

**ç‰¹ç‚¹**:
- è‡ªåŠ¨æ·»åŠ User-Agentï¼ˆé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼‰
- 20ç§’è¶…æ—¶ä¿æŠ¤
- è‡ªåŠ¨é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

**ç¤ºä¾‹**:
```python
response = safe_request_get("https://example.com")
if response:
    print(response.text)
```

### 2. PDFæ–‡æœ¬æå– (`download_pdf_to_text`)

**åŠŸèƒ½**: ä¸‹è½½PDFå¹¶æå–çº¯æ–‡æœ¬

**æ”¯æŒ**:
- æµå¼ä¸‹è½½ï¼Œæ”¯æŒå¤§æ–‡ä»¶
- è‡ªåŠ¨ç”Ÿæˆä¸´æ—¶æ–‡ä»¶åï¼ˆSHA1å“ˆå¸Œï¼‰
- æå–åè‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶

**ç¤ºä¾‹**:
```python
text = download_pdf_to_text("https://example.com/report.pdf")
print(text[:500])  # æ‰“å°å‰500å­—ç¬¦
```

### 3. HTMLå†…å®¹æå– (`html_to_text`)

**åŠŸèƒ½**: ä»HTMLä¸­æå–æ ‡é¢˜å’Œæ­£æ–‡

**æ¸…æ´—ç­–ç•¥**:
- ç§»é™¤è„šæœ¬ã€æ ·å¼ã€å¯¼èˆªæ 
- ç§»é™¤å¹¿å‘Šå’Œå™ªå£°å…ƒç´ 
- æ™ºèƒ½å®šä½ä¸»å†…å®¹åŒºåŸŸ
- æ¸…ç†å¤šä½™ç©ºç™½å­—ç¬¦

**ç¤ºä¾‹**:
```python
html = "<html><head><title>ç¤ºä¾‹</title></head><body>å†…å®¹</body></html>"
title, text = html_to_text(html)
print(f"æ ‡é¢˜: {title}, å†…å®¹: {text}")
```

### 4. æ–‡æœ¬åˆ†å‰² (`split_text_into_chunks`)

**åŠŸèƒ½**: å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunk

**å‚æ•°**:
- `chunk_size_chars`: æ¯ä¸ªchunkçš„å­—ç¬¦æ•°ï¼ˆæ¨è1800ï¼‰
- `chunk_overlap_chars`: é‡å å­—ç¬¦æ•°ï¼ˆæ¨è200ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²ï¼Ÿ**
- Embeddingæ¨¡å‹æœ‰è¾“å…¥é•¿åº¦é™åˆ¶ï¼ˆé€šå¸¸512-8192 tokensï¼‰
- æ›´å°çš„chunkæé«˜æ£€ç´¢ç²¾åº¦
- é‡å éƒ¨åˆ†é˜²æ­¢ä¿¡æ¯åœ¨è¾¹ç•Œä¸¢å¤±

**ç¤ºä¾‹**:
```python
text = "å¾ˆé•¿çš„æ–‡æœ¬..." * 1000
chunks = split_text_into_chunks(text, chunk_size_chars=1800, chunk_overlap_chars=200)
print(f"åˆ†å‰²æˆ {len(chunks)} ä¸ªchunk")
```

### 5. å…ƒæ•°æ®æå– (`generate_metadata`)

**åŠŸèƒ½**: è‡ªåŠ¨ä»æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®æ ‡ç­¾

**æå–å†…å®¹**:
- CVEç¼–å·: `CVE-2021-44228`
- MITRE ATT&CK ID: `T1566`
- CWEç¼–å·: `CWE-79`
- æ—¥æœŸ: `2021-12-10`
- åˆ†ç±»æ ‡ç­¾: SQLæ³¨å…¥ã€XSSã€é’“é±¼ç­‰

**ç¤ºä¾‹**:
```python
text = "CVE-2021-44228æ˜¯Log4Shellæ¼æ´ï¼Œå±äºT1190æ”»å‡»æŠ€æœ¯ã€‚"
meta = generate_metadata(text, url="https://example.com", title="Log4Shell")
print(meta)
# è¾“å‡º: {'cves': ['CVE-2021-44228'], 'mitre_techniques': ['T1190'], ...}
```

### 6. æ•°æ®åº“æ“ä½œ

#### åˆ›å»ºæ•°æ®åº“ (`create_database`)
```python
db_name, metric = create_database(metric_type="L2")
print(f"æ•°æ®åº“å: {db_name}")
```

#### æ‰¹é‡ä¸Šä¼  (`upload_chunks`)
```python
chunks = [
    {"file": "æ–‡æœ¬å†…å®¹1", "metadata": {"title": "æ ‡é¢˜1"}},
    {"file": "æ–‡æœ¬å†…å®¹2", "metadata": {"title": "æ ‡é¢˜2"}},
]
file_ids = upload_chunks(db_name, chunks)
print(f"ä¸Šä¼ äº† {len(file_ids)} ä¸ªæ–‡ä»¶")
```

### 7. çˆ¬è™«å‡½æ•°

#### MITRE ATT&CK (`crawl_mitre_attack`)
```python
pages = crawl_mitre_attack(max_pages=50)
```

#### OWASP Top 10 (`crawl_owasp_top10`)
```python
pages = crawl_owasp_top10()
```

#### é€šç”¨çˆ¬è™« (`crawl_generic_urls`)
```python
urls = [
    "https://www.freebuf.com/articles/web/123456.html",
    "https://example.com/security-report.pdf",
]
pages = crawl_generic_urls(urls)
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: è¿è¡Œæ—¶æç¤º"ModuleNotFoundError"

**é—®é¢˜**: ç¼ºå°‘ä¾èµ–åº“

**è§£å†³**:
```bash
pip install requests beautifulsoup4 pdfminer.six
```

### Q2: çˆ¬å–æ—¶æç¤º"GET https://xxx å¤±è´¥"

**å¯èƒ½åŸå› **:
1. ç½‘ç»œè¿æ¥é—®é¢˜
2. ç›®æ ‡ç½‘ç«™ä¸å¯è®¿é—®
3. è¢«åçˆ¬è™«æ‹¦æˆª

**è§£å†³**:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. å°è¯•åœ¨æµè§ˆå™¨ä¸­è®¿é—®è¯¥URL
3. å¢åŠ è¯·æ±‚å»¶è¿Ÿ `time.sleep(1)`
4. æ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç†

### Q3: PDFæå–å¤±è´¥

**å¯èƒ½åŸå› **:
1. PDFæ–‡ä»¶æŸå
2. PDFæ˜¯æ‰«æç‰ˆï¼ˆå›¾ç‰‡ï¼‰
3. PDFæœ‰å¯†ç ä¿æŠ¤

**è§£å†³**:
1. åœ¨æµè§ˆå™¨ä¸­ç¡®è®¤PDFå¯ä»¥æ‰“å¼€
2. æ‰«æç‰ˆPDFéœ€è¦OCRæŠ€æœ¯
3. å¯†ç ä¿æŠ¤çš„PDFéœ€è¦å…ˆè§£å¯†

### Q4: ä¸Šä¼ æ—¶æç¤º"401 Unauthorized"

**é—®é¢˜**: Tokenæ— æ•ˆæˆ–è¿‡æœŸ

**è§£å†³**:
1. æ£€æŸ¥TOKENæ˜¯å¦æ­£ç¡®
2. è”ç³»ç®¡ç†å‘˜è·å–æ–°Token

### Q5: çˆ¬å–é€Ÿåº¦å¾ˆæ…¢

**åŸå› **: ä¸ºäº†éµå®ˆçˆ¬è™«ç¤¼ä»ªï¼Œä»£ç ä¸­è®¾ç½®äº†å»¶è¿Ÿ

**è°ƒæ•´**:
```python
# åœ¨çˆ¬è™«å‡½æ•°ä¸­ä¿®æ”¹å»¶è¿Ÿæ—¶é—´
time.sleep(0.3)  # ä»0.6ç§’æ”¹ä¸º0.3ç§’ï¼ˆä¸æ¨èå¤ªå°ï¼‰
```

### Q6: å†…å­˜å ç”¨è¿‡é«˜

**åŸå› **: ä¸€æ¬¡æ€§åŠ è½½å¤ªå¤šé¡µé¢åˆ°å†…å­˜

**è§£å†³**:
1. å‡å°‘ `max_pages` å‚æ•°
2. åˆ†æ‰¹æ¬¡è¿è¡Œ
3. å¢åŠ æœºå™¨å†…å­˜

---

## ğŸ“ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰æ•°æ®æº

#### æ–¹å¼1: æ·»åŠ URLåˆ—è¡¨
åœ¨ `pipeline_demo()` å‡½æ•°ä¸­çš„ `extra_urls` æ·»åŠ ï¼š

```python
extra_urls = [
    "https://www.freebuf.com/articles/web/123456.html",
    "https://www.anquanke.com/post/id/234567",
    "https://example.com/security-report.pdf",
]
```

#### æ–¹å¼2: ç¼–å†™æ–°çš„çˆ¬è™«å‡½æ•°

```python
def crawl_your_site():
    """è‡ªå®šä¹‰çˆ¬è™«å‡½æ•°"""
    urls = ["https://your-site.com/page1", "https://your-site.com/page2"]
    return crawl_generic_urls(urls)

# åœ¨ pipeline_demo() ä¸­è°ƒç”¨
your_pages = crawl_your_site()
all_pages = mitre_pages + owasp_pages + your_pages
```

### è°ƒæ•´Chunkå¤§å°

æ ¹æ®ä½ çš„embeddingæ¨¡å‹è°ƒæ•´ï¼š

```python
# ç¤ºä¾‹ï¼šå¦‚æœä½ çš„æ¨¡å‹æ”¯æŒæ›´é•¿çš„è¾“å…¥
chunks = split_text_into_chunks(
    text, 
    chunk_size_chars=3000,      # å¢åŠ åˆ°3000å­—ç¬¦
    chunk_overlap_chars=300     # å¢åŠ é‡å åˆ°300å­—ç¬¦
)
```

**æ¨èé…ç½®**:
- çŸ­æ–‡æœ¬æ¨¡å‹ (512 tokens): `chunk_size_chars=800`
- ä¸­ç­‰æ¨¡å‹ (2048 tokens): `chunk_size_chars=1800` (é»˜è®¤)
- é•¿æ–‡æœ¬æ¨¡å‹ (8192 tokens): `chunk_size_chars=6000`

### å¢é‡æ›´æ–°

å¦‚æœæƒ³å®šæœŸæ›´æ–°æ•°æ®åº“ï¼š

```python
# 1. ä½¿ç”¨ç°æœ‰æ•°æ®åº“ï¼ˆä¸åˆ›å»ºæ–°çš„ï¼‰
db_name = "student_Group12_existing_db"

# 2. åªçˆ¬å–æ–°å†…å®¹
new_urls = ["https://new-article.com"]
new_pages = crawl_generic_urls(new_urls)

# 3. å¤„ç†å¹¶ä¸Šä¼ 
# ... (åç»­æ­¥éª¤ä¸pipeline_demoç›¸åŒ)
```

### è¿‡æ»¤å’Œç­›é€‰

#### æŒ‰åˆ†ç±»è¿‡æ»¤
```python
# åªä¿ç•™åŒ…å«SQLæ³¨å…¥çš„å†…å®¹
filtered_pages = [
    p for p in all_pages 
    if 'sql_injection' in p.get('meta', {}).get('categories', [])
]
```

#### æŒ‰é•¿åº¦è¿‡æ»¤
```python
# åªä¿ç•™é•¿åº¦å¤§äº1000å­—ç¬¦çš„é¡µé¢
filtered_pages = [p for p in all_pages if len(p['text']) > 1000]
```

### å¹¶è¡Œçˆ¬å–ï¼ˆè¿›é˜¶ï¼‰

ä½¿ç”¨å¤šçº¿ç¨‹åŠ é€Ÿçˆ¬å–ï¼š

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def crawl_with_thread_pool(urls, max_workers=5):
    """å¹¶è¡Œçˆ¬å–å¤šä¸ªURL"""
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(safe_request_get, url): url for url in urls}
        for future in as_completed(futures):
            url = futures[future]
            try:
                response = future.result()
                if response:
                    title, text = html_to_text(response.text)
                    results.append({"url": url, "title": title, "text": text})
            except Exception as e:
                logging.error(f"çˆ¬å–å¤±è´¥ {url}: {e}")
    return results
```

**æ³¨æ„**: å¹¶è¡Œçˆ¬å–æ—¶è¦æ›´åŠ æ³¨æ„è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¢«å°ç¦ã€‚

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. åˆç†è®¾ç½®çˆ¬å–æ•°é‡
- åˆå­¦è€…: 30-50é¡µï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- ä¸­çº§: 100-200é¡µï¼ˆå®ç”¨è§„æ¨¡ï¼‰
- é«˜çº§: 500+é¡µï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

### 2. æ‰¹é‡ä¸Šä¼ å¤§å°
- å°æ–‡ä»¶ (<1KB): `UPLOAD_BATCH = 50`
- ä¸­ç­‰æ–‡ä»¶ (1-10KB): `UPLOAD_BATCH = 20` (é»˜è®¤10)
- å¤§æ–‡ä»¶ (>10KB): `UPLOAD_BATCH = 5`

### 3. è¯·æ±‚å»¶è¿Ÿ
- å›½é™…ç«™ç‚¹: 1-2ç§’
- å›½å†…ç«™ç‚¹: 0.5-1ç§’
- æœ¬åœ°æµ‹è¯•: 0.1-0.3ç§’

### 4. å†…å­˜ç®¡ç†
```python
# åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰é¡µé¢
def process_in_batches(pages, batch_size=100):
    for i in range(0, len(pages), batch_size):
        batch = pages[i:i+batch_size]
        # å¤„ç†batch
        yield batch
```

---

## ğŸ”’ å®‰å…¨ä¸åˆè§„

### çˆ¬è™«ç¤¼ä»ª
1. **éµå®ˆ robots.txt**: æ£€æŸ¥ç½‘ç«™æ˜¯å¦å…è®¸çˆ¬å–
2. **è¯·æ±‚é¢‘ç‡**: ä¸è¦è¿‡å¿«ï¼Œå»ºè®®0.5-1ç§’/è¯·æ±‚
3. **User-Agent**: æ ‡è¯†ä½ çš„çˆ¬è™«èº«ä»½
4. **é”™è¯¯å¤„ç†**: ä¸è¦æ— é™é‡è¯•

### æ³•å¾‹åˆè§„
1. **ä¸ªäººä½¿ç”¨**: ç”¨äºå­¦ä¹ å’Œç ”ç©¶
2. **å•†ä¸šä½¿ç”¨**: éœ€è¦è·å¾—ç½‘ç«™æˆæƒ
3. **ç‰ˆæƒå°Šé‡**: ä¸è¦ä¾µçŠ¯å†…å®¹ç‰ˆæƒ
4. **éšç§ä¿æŠ¤**: ä¸è¦çˆ¬å–ä¸ªäººéšç§ä¿¡æ¯

### ä»£ç ä¸­çš„åˆè§„æªæ–½
```python
# 1. User-Agent æ ‡è¯†
headers.setdefault("User-Agent", "Mozilla/5.0 (compatible; CyberSecRAG/1.0)")

# 2. è¯·æ±‚å»¶è¿Ÿ
time.sleep(0.6)  # ç¤¼è²Œç­‰å¾…

# 3. é”™è¯¯å¤„ç†
try:
    response = requests.get(url, timeout=20)
except Exception as e:
    logging.warning(f"è¯·æ±‚å¤±è´¥: {e}")
    return None
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜:
1. æ£€æŸ¥æœ¬æ–‡æ¡£çš„"å¸¸è§é—®é¢˜"éƒ¨åˆ†
2. æŸ¥çœ‹ä»£ç ä¸­çš„è¯¦ç»†æ³¨é‡Š
3. é˜…è¯» `æ•°æ®æºæ¨è.md` äº†è§£æ•°æ®æº
4. æ£€æŸ¥åç«¯APIæ–‡æ¡£

---

## ğŸ¯ ä¸‹ä¸€æ­¥

å®ŒæˆPipelineå:
1. **æµ‹è¯•æœç´¢**: ä½¿ç”¨ä¸åŒæŸ¥è¯¢æµ‹è¯•æ£€ç´¢æ•ˆæœ
2. **è¯„ä¼°è´¨é‡**: æ£€æŸ¥è¿”å›ç»“æœçš„ç›¸å…³æ€§
3. **ä¼˜åŒ–è°ƒæ•´**: æ ¹æ®æ•ˆæœè°ƒæ•´chunkå¤§å°ã€æ•°æ®æº
4. **é›†æˆåº”ç”¨**: å°†æ•°æ®åº“é›†æˆåˆ°ä½ çš„RAGåº”ç”¨ä¸­

**é›†æˆç¤ºä¾‹**:
```python
# åœ¨ä½ çš„RAGåº”ç”¨ä¸­ä½¿ç”¨æ„å»ºå¥½çš„æ•°æ®åº“
import requests

def rag_query(user_question, db_name):
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    search_response = requests.post(
        f"{BASE_URL}/databases/{db_name}/search",
        json={
            "token": TOKEN,
            "query": user_question,
            "top_k": 5,
            "metric_type": "L2"
        }
    )
    
    results = search_response.json()["results"]
    
    # 2. æ„é€ prompt
    context = "\n\n".join([r["file"] for r in results])
    prompt = f"åŸºäºä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”é—®é¢˜:\n\n{context}\n\né—®é¢˜: {user_question}"
    
    # 3. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
    # ... (ä½¿ç”¨ä½ çš„LLM API)
    
    return answer
```

ç¥ä½ æ„å»ºæˆåŠŸï¼ğŸ‰

