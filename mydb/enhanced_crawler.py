"""
å¢å¼ºç‰ˆç½‘ç»œå®‰å…¨çŸ¥è¯†åº“çˆ¬å–ç³»ç»Ÿ
é›†æˆæ”¹è¿›çš„æ–‡æœ¬æ¸…æ´—ã€æ™ºèƒ½åˆ†å—ã€PDFå¤„ç†ç­‰åŠŸèƒ½
åŒ…å«ä¸°å¯Œçš„ä¸­è‹±æ–‡æ•°æ®æº
"""

import sys
import os
import time
import logging
import requests
from typing import List, Dict, Tuple, Optional
from urllib.parse import urlparse, urljoin

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from bs4 import BeautifulSoup
from pdfminer.high_level import extract_text
import hashlib

# å¯¼å…¥æ”¹è¿›çš„æ¨¡å—
from mydb.text_cleaner import TextCleaner, ContentQualityEvaluator
from mydb.smart_chunker import SmartChunker
from mydb.createdb_pipeline import (
    BASE_URL, TOKEN, METRIC_TYPE,
    safe_request_get, generate_metadata,
    create_database, upload_chunks
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# PDFä¸´æ—¶ç›®å½•
PDF_TMP_DIR = os.path.join(os.getenv("TEMP", "."), "pdf_cache")
os.makedirs(PDF_TMP_DIR, exist_ok=True)


class EnhancedCrawler:
    """å¢å¼ºç‰ˆçˆ¬è™«ï¼Œæ”¯æŒç½‘é¡µå’ŒPDFï¼Œé›†æˆæ–‡æœ¬æ¸…æ´—å’Œè´¨é‡æ§åˆ¶"""
    
    def __init__(
        self,
        chunk_size: int = 1500,
        chunk_overlap: int = 150,
        min_chunk_size: int = 100,
        quality_threshold: float = 0.4,
        aggressive_cleaning: bool = False
    ):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆçˆ¬è™«
        
        Args:
            chunk_size: ç›®æ ‡chunkå¤§å°
            chunk_overlap: chunké‡å å¤§å°
            min_chunk_size: æœ€å°chunkå¤§å°
            quality_threshold: è´¨é‡é˜ˆå€¼ï¼ˆ0-1ï¼‰
            aggressive_cleaning: æ˜¯å¦ä½¿ç”¨æ¿€è¿›æ¸…æ´—
        """
        self.text_cleaner = TextCleaner()
        self.quality_evaluator = ContentQualityEvaluator()
        self.chunker = SmartChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_chunk_size=min_chunk_size
        )
        self.quality_threshold = quality_threshold
        self.aggressive_cleaning = aggressive_cleaning
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_urls': 0,
            'successful_pages': 0,
            'failed_pages': 0,
            'low_quality_pages': 0,
            'total_chunks': 0,
            'filtered_chunks': 0,
            'final_chunks': 0,
            'pdf_count': 0,
            'html_count': 0
        }
    
    def download_pdf_to_text(self, url: str) -> str:
        """
        ä¸‹è½½PDFå¹¶æå–æ–‡æœ¬
        
        Args:
            url: PDFæ–‡ä»¶URL
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        logging.info(f"ä¸‹è½½PDF: {url}")
        self.stats['pdf_count'] += 1
        
        r = safe_request_get(url, stream=True)
        if not r:
            return ""
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        fn = os.path.join(PDF_TMP_DIR, hashlib.sha1(url.encode()).hexdigest() + ".pdf")
        
        # ä¸‹è½½æ–‡ä»¶
        with open(fn, "wb") as f:
            for chunk in r.iter_content(1024*16):
                if chunk:
                    f.write(chunk)
        
        try:
            # æå–æ–‡æœ¬
            text = extract_text(fn)
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(fn)
            except:
                pass
            
            return text
        except Exception as e:
            logging.warning(f"PDFæå–å¤±è´¥: {e}")
            return ""
    
    def html_to_text(self, html: str) -> Tuple[str, str]:
        """
        ä»HTMLä¸­æå–æ ‡é¢˜å’Œæ­£æ–‡ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        
        Args:
            html: HTMLæºä»£ç 
            
        Returns:
            (title, text) å…ƒç»„
        """
        soup = BeautifulSoup(html, "html.parser")
        
        # æå–æ ‡é¢˜
        title = (soup.title.string.strip() if soup.title and soup.title.string else "") or ""
        
        # ç§»é™¤æ— ç”¨æ ‡ç­¾
        for s in soup(["script", "style", "nav", "header", "footer", "aside", "form"]):
            s.decompose()
        
        # ç§»é™¤å™ªå£°å…ƒç´ 
        noise_selectors = [
            '.breadcrumb', '.nav', '.footer', '#footer', '.sidebar',
            '.ad', '.advert', '.cookie', '.menu', '.banner',
            '.social-share', '.comment', '.related-posts'
        ]
        for sel in noise_selectors:
            for node in soup.select(sel):
                node.decompose()
        
        # æ™ºèƒ½å®šä½ä¸»è¦å†…å®¹
        main = soup.find("main") or soup.find("article")
        
        if not main:
            candidates = soup.find_all(["div", "section"], recursive=True)
            if candidates:
                main = max(candidates, key=lambda d: len(d.get_text(separator=" ", strip=True)) if d else 0)
            else:
                main = soup.body
        
        # æå–æ–‡æœ¬
        text = main.get_text(separator="\n", strip=True) if main else soup.get_text(separator="\n", strip=True)
        
        return title, text
    
    def process_url(self, url: str) -> List[Dict]:
        """
        å¤„ç†å•ä¸ªURLï¼ˆç½‘é¡µæˆ–PDFï¼‰
        
        Args:
            url: è¦å¤„ç†çš„URL
            
        Returns:
            å¤„ç†åçš„chunkåˆ—è¡¨
        """
        self.stats['total_urls'] += 1
        
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸ºPDF
            if url.lower().endswith('.pdf'):
                # å¤„ç†PDF
                text = self.download_pdf_to_text(url)
                title = url.split('/')[-1].replace('.pdf', '')
                
                if not text or len(text) < 100:
                    logging.warning(f"PDFå†…å®¹è¿‡çŸ­: {url}")
                    self.stats['failed_pages'] += 1
                    return []
            else:
                # å¤„ç†HTML
                self.stats['html_count'] += 1
                response = safe_request_get(url)
                if not response:
                    logging.warning(f"çˆ¬å–å¤±è´¥: {url}")
                    self.stats['failed_pages'] += 1
                    return []
                
                title, text = self.html_to_text(response.text)
                
                if not text or len(text) < 100:
                    logging.warning(f"æ–‡æœ¬è¿‡çŸ­: {url}")
                    self.stats['failed_pages'] += 1
                    return []
            
            # æ¸…æ´—æ–‡æœ¬
            cleaned_text = self.text_cleaner.clean_text(text, aggressive=self.aggressive_cleaning)
            if not cleaned_text:
                logging.warning(f"æ¸…æ´—åä¸ºç©º: {url}")
                self.stats['failed_pages'] += 1
                return []
            
            # è´¨é‡è¯„ä¼°
            quality_scores = self.quality_evaluator.calculate_quality_score(cleaned_text)
            if quality_scores['overall'] < self.quality_threshold:
                logging.info(f"è´¨é‡ä¸è¾¾æ ‡ (åˆ†æ•°: {quality_scores['overall']:.2f}): {url}")
                self.stats['low_quality_pages'] += 1
                return []
            
            # æ™ºèƒ½åˆ†å—
            chunks = self.chunker.chunk_text(cleaned_text, deduplicate=True)
            self.stats['total_chunks'] += len(chunks)
            
            if not chunks:
                logging.warning(f"åˆ†å—åä¸ºç©º: {url}")
                self.stats['failed_pages'] += 1
                return []
            
            # ä¸ºæ¯ä¸ªchunkç”Ÿæˆå…ƒæ•°æ®
            upload_items = []
            for i, chunk_text in enumerate(chunks):
                # è¯„ä¼°chunkè´¨é‡
                chunk_quality = self.quality_evaluator.calculate_quality_score(chunk_text)
                
                # è¿‡æ»¤ä½è´¨é‡chunk
                if chunk_quality['overall'] < self.quality_threshold:
                    self.stats['filtered_chunks'] += 1
                    continue
                
                # ç”Ÿæˆå…ƒæ•°æ®
                metadata = generate_metadata(chunk_text, url, title)
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                metadata['quality_score'] = round(chunk_quality['overall'], 4)
                metadata['chunk_index'] = i
                metadata['total_chunks'] = len(chunks)
                metadata['source_type'] = 'pdf' if url.lower().endswith('.pdf') else 'html'
                
                upload_items.append({
                    'file': chunk_text,
                    'metadata': metadata
                })
            
            self.stats['successful_pages'] += 1
            self.stats['final_chunks'] += len(upload_items)
            
            logging.info(f"âœ“ å¤„ç†æˆåŠŸ: {url} | è´¨é‡: {quality_scores['overall']:.2f} | "
                        f"Chunks: {len(upload_items)}/{len(chunks)}")
            
            return upload_items
            
        except Exception as e:
            logging.error(f"å¤„ç†å¤±è´¥ {url}: {e}")
            self.stats['failed_pages'] += 1
            return []
    
    def process_urls(self, urls: List[str], delay: float = 1.0) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†URLåˆ—è¡¨
        
        Args:
            urls: URLåˆ—è¡¨
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            æ‰€æœ‰chunkçš„åˆ—è¡¨
        """
        all_chunks = []
        
        for i, url in enumerate(urls, 1):
            logging.info(f"\nå¤„ç†è¿›åº¦: {i}/{len(urls)}")
            chunks = self.process_url(url)
            all_chunks.extend(chunks)
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            if i < len(urls):  # æœ€åä¸€ä¸ªURLä¸éœ€è¦å»¶è¿Ÿ
                time.sleep(delay)
        
        return all_chunks
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logging.info("\n" + "=" * 80)
        logging.info("å¤„ç†ç»Ÿè®¡:")
        logging.info("=" * 80)
        logging.info(f"æ€»URLæ•°: {self.stats['total_urls']}")
        logging.info(f"  - HTMLé¡µé¢: {self.stats['html_count']}")
        logging.info(f"  - PDFæ–‡ä»¶: {self.stats['pdf_count']}")
        logging.info(f"æˆåŠŸå¤„ç†: {self.stats['successful_pages']}")
        logging.info(f"å¤±è´¥é¡µé¢: {self.stats['failed_pages']}")
        logging.info(f"ä½è´¨é‡é¡µé¢: {self.stats['low_quality_pages']}")
        logging.info(f"ç”Ÿæˆchunkæ€»æ•°: {self.stats['total_chunks']}")
        logging.info(f"è¿‡æ»¤chunkæ•°: {self.stats['filtered_chunks']}")
        logging.info(f"æœ€ç»ˆchunkæ•°: {self.stats['final_chunks']}")
        
        if self.stats['total_urls'] > 0:
            success_rate = self.stats['successful_pages'] / self.stats['total_urls'] * 100
            logging.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        if self.stats['total_chunks'] > 0:
            filter_rate = self.stats['filtered_chunks'] / self.stats['total_chunks'] * 100
            logging.info(f"è¿‡æ»¤ç‡: {filter_rate:.1f}%")
        
        logging.info("=" * 80)


def get_cybersecurity_urls() -> Dict[str, List[str]]:
    """
    è·å–ç½‘ç»œå®‰å…¨ç›¸å…³çš„é«˜è´¨é‡æ•°æ®æºURL
    åŒ…å«ä¸­è‹±æ–‡ç½‘ç«™
    
    Returns:
        æŒ‰ç±»åˆ«åˆ†ç»„çš„URLå­—å…¸
    """
    urls = {
        # ===== å›½é™…æƒå¨æ ‡å‡†å’ŒçŸ¥è¯†åº“ =====
        # 'standards': [
        #     # OWASP Top 10 (Webåº”ç”¨å®‰å…¨)
        #     "https://owasp.org/www-project-top-ten/",
        #     "https://owasp.org/Top10/A01_2021-Broken_Access_Control/",
        #     "https://owasp.org/Top10/A02_2021-Cryptographic_Failures/",
        #     "https://owasp.org/Top10/A03_2021-Injection/",
            
        #     # MITRE ATT&CK (æ”»å‡»æŠ€æœ¯)
        #     "https://attack.mitre.org/techniques/T1190/",  # Exploit Public-Facing Application
        #     "https://attack.mitre.org/techniques/T1059/",  # Command and Scripting Interpreter
        #     "https://attack.mitre.org/techniques/T1078/",  # Valid Accounts
        #     "https://attack.mitre.org/techniques/T1566/",  # Phishing
            
        #     # CWE (é€šç”¨å¼±ç‚¹æšä¸¾)
        #     "https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html",
        # ],
        
        # ===== æŠ€æœ¯æ•™ç¨‹å’Œå®æˆ˜ =====
        # 'tutorials': [
        #     # PortSwigger Web Security Academy
        #     "https://portswigger.net/web-security/sql-injection",
        #     "https://portswigger.net/web-security/cross-site-scripting",
        #     "https://portswigger.net/web-security/csrf",
        #     "https://portswigger.net/web-security/xxe",
            
        #     # OWASP Cheat Sheet
        #     "https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html",
        #     "https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html",
        # ],
        
        # # ===== ä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼ˆç½‘ç»œå®‰å…¨ç›¸å…³ï¼‰ =====
        # 'wikipedia_zh': [
        #     "https://zh.wikipedia.org/wiki/SQLæ³¨å…¥",
        #     "https://zh.wikipedia.org/wiki/è·¨ç¶²ç«™æŒ‡ä»¤ç¢¼",
        #     "https://zh.wikipedia.org/wiki/è·¨ç«™è¯·æ±‚ä¼ªé€ ",
        #     "https://zh.wikipedia.org/wiki/æ‹’ç»æœåŠ¡æ”»å‡»",
        #     "https://zh.wikipedia.org/wiki/é’“é±¼å¼æ”»å‡»",
        #     "https://zh.wikipedia.org/wiki/å‹’ç´¢è½¯ä»¶",
        #     "https://zh.wikipedia.org/wiki/é˜²ç«å¢™",
        #     "https://zh.wikipedia.org/wiki/å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ",
        #     "https://zh.wikipedia.org/wiki/åŠ å¯†",
        #     "https://zh.wikipedia.org/wiki/æ•°å­—ç­¾å",
        #     "https://zh.wikipedia.org/wiki/å…¬å¼€å¯†é’¥åŠ å¯†",
        #     "https://zh.wikipedia.org/wiki/æ¸—é€æµ‹è¯•",
        # ],
        
        # # ===== è‹±æ–‡ç»´åŸºç™¾ç§‘ï¼ˆç½‘ç»œå®‰å…¨ç›¸å…³ï¼‰ =====
        # 'wikipedia_en': [
        #     "https://en.wikipedia.org/wiki/SQL_injection",
        #     "https://en.wikipedia.org/wiki/Cross-site_scripting",
        #     "https://en.wikipedia.org/wiki/Cross-site_request_forgery",
        #     "https://en.wikipedia.org/wiki/Denial-of-service_attack",
        #     "https://en.wikipedia.org/wiki/Phishing",
        #     "https://en.wikipedia.org/wiki/Ransomware",
        #     "https://en.wikipedia.org/wiki/Penetration_test",
        #     "https://en.wikipedia.org/wiki/Computer_security",
        # ],
        
        # ===== ä¸­æ–‡å®‰å…¨èµ„è®¯å’ŒæŠ€æœ¯åšå®¢ =====
        'chinese_blogs': [
            # æ³¨æ„ï¼šè¿™äº›URLéœ€è¦æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©å…·ä½“æ–‡ç« 
            # è¿™é‡Œæä¾›ä¸€äº›ä¸»é¡µå’Œå¸¸è§æŠ€æœ¯æ–‡ç« ç±»å‹çš„ç¤ºä¾‹
            
            # å…ˆçŸ¥ç¤¾åŒºï¼ˆé˜¿é‡Œäº‘ï¼‰- éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
            "https://xz.aliyun.com/",
            
            # å®‰å…¨å®¢ - éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
            "https://www.anquanke.com/",
            
            # FreeBuf - éœ€è¦æ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URL
            "https://www.freebuf.com/",
        ],
        
#         # ===== æ”¿åºœå’Œæœºæ„èµ„æº =====
#         'government': [
#             # NIST ç½‘ç»œå®‰å…¨èµ„æº
#             "https://csrc.nist.gov/",
            
#             # US-CERT
#             "https://www.cisa.gov/news-events/cybersecurity-advisories",
#         ],
#         'laws': [
#             "http://www.npc.gov.cn/zgrdw/npc/xinwen/2016-11/07/content_2001605.htm",
#            "http://www.npc.gov.cn/c2/c30834/202106/t20210610_311888.html",
#            "http://www.npc.gov.cn/npc/c2/c30834/202108/t20210820_313088.html",
#            "https://www.gov.cn/gongbao/content/2021/content_5636138.htm",

#         ],
#         'wikipedia': [
#     # æ”»å‡»æŠ€æœ¯
#     "https://zh.wikipedia.org/wiki/SQLæ³¨å…¥",
#     "https://zh.wikipedia.org/wiki/è·¨ç¶²ç«™æŒ‡ä»¤ç¢¼",
#     "https://zh.wikipedia.org/wiki/è·¨ç«™è¯·æ±‚ä¼ªé€ ",
#     "https://zh.wikipedia.org/wiki/ç¼“å†²åŒºæº¢å‡º",
#     "https://zh.wikipedia.org/wiki/æ‹’ç»æœåŠ¡æ”»å‡»",
#     "https://zh.wikipedia.org/wiki/é’“é±¼å¼æ”»å‡»",
#     "https://zh.wikipedia.org/wiki/ä¸­é—´äººæ”»å‡»",
#     "https://zh.wikipedia.org/wiki/ä¼šè¯åŠ«æŒ",
    
#     # æ¶æ„è½¯ä»¶
#     "https://zh.wikipedia.org/wiki/å‹’ç´¢è½¯ä»¶",
#     "https://zh.wikipedia.org/wiki/ç‰¹æ´›ä¼Šæœ¨é©¬_(ç”µè„‘)",
#     "https://zh.wikipedia.org/wiki/è®¡ç®—æœºç—…æ¯’",
#     "https://zh.wikipedia.org/wiki/è®¡ç®—æœºè •è™«",
#     "https://zh.wikipedia.org/wiki/é—´è°è½¯ä»¶",
#     "https://zh.wikipedia.org/wiki/Rootkit",
    
#     # é˜²å¾¡æŠ€æœ¯
#     "https://zh.wikipedia.org/wiki/é˜²ç«å¢™",
#     "https://zh.wikipedia.org/wiki/å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ",
#     "https://zh.wikipedia.org/wiki/å…¥ä¾µé¢„é˜²ç³»ç»Ÿ",
#     "https://zh.wikipedia.org/wiki/è™šæ‹Ÿä¸“ç”¨ç½‘",
#     "https://zh.wikipedia.org/wiki/Webåº”ç”¨ç¨‹åºé˜²ç«å¢™",
    
#     # åŠ å¯†æŠ€æœ¯
#     "https://zh.wikipedia.org/wiki/åŠ å¯†",
#     "https://zh.wikipedia.org/wiki/å…¬å¼€å¯†é’¥åŠ å¯†",
#     "https://zh.wikipedia.org/wiki/æ•°å­—ç­¾å",
#     "https://zh.wikipedia.org/wiki/å‚³è¼¸å±¤å®‰å…¨æ€§å”å®š",
#     "https://zh.wikipedia.org/wiki/å®‰å…¨å¥—æ¥å±‚",
    
#     # å®‰å…¨æ¦‚å¿µ
#     "https://zh.wikipedia.org/wiki/ä¿¡æ¯å®‰å…¨",
#     "https://zh.wikipedia.org/wiki/ç½‘ç»œå®‰å…¨",
#     "https://zh.wikipedia.org/wiki/æ¸—é€æµ‹è¯•",
#     "https://zh.wikipedia.org/wiki/æ¼æ´æ‰«æå™¨",
#     "https://zh.wikipedia.org/wiki/ç¤¾ä¼šå·¥ç¨‹å­¦",
#     "https://zh.wikipedia.org/wiki/é›¶æ—¥æ”»å‡»",
# ]
    }
    
    return urls


def process_pdf_file(pdf_path: str, crawler: EnhancedCrawler) -> List[Dict]:
    """
    å¤„ç†æœ¬åœ°PDFæ–‡ä»¶
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        crawler: çˆ¬è™«å®ä¾‹
        
    Returns:
        å¤„ç†åçš„chunkåˆ—è¡¨
    """
    logging.info(f"å¤„ç†æœ¬åœ°PDF: {pdf_path}")
    
    try:
        # æå–æ–‡æœ¬
        text = extract_text(pdf_path)
        
        if not text or len(text) < 100:
            logging.warning(f"PDFå†…å®¹è¿‡çŸ­: {pdf_path}")
            return []
        
        # è·å–æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
        title = os.path.basename(pdf_path).replace('.pdf', '')
        
        # æ¸…æ´—æ–‡æœ¬
        cleaned_text = crawler.text_cleaner.clean_text(text, aggressive=crawler.aggressive_cleaning)
        if not cleaned_text:
            logging.warning(f"æ¸…æ´—åä¸ºç©º: {pdf_path}")
            return []
        
        # è´¨é‡è¯„ä¼°
        quality_scores = crawler.quality_evaluator.calculate_quality_score(cleaned_text)
        if quality_scores['overall'] < crawler.quality_threshold:
            logging.info(f"è´¨é‡ä¸è¾¾æ ‡ (åˆ†æ•°: {quality_scores['overall']:.2f}): {pdf_path}")
            return []
        
        # æ™ºèƒ½åˆ†å—
        chunks = crawler.chunker.chunk_text(cleaned_text, deduplicate=True)
        
        if not chunks:
            logging.warning(f"åˆ†å—åä¸ºç©º: {pdf_path}")
            return []
        
        # ä¸ºæ¯ä¸ªchunkç”Ÿæˆå…ƒæ•°æ®
        upload_items = []
        for i, chunk_text in enumerate(chunks):
            # è¯„ä¼°chunkè´¨é‡
            chunk_quality = crawler.quality_evaluator.calculate_quality_score(chunk_text)
            
            # è¿‡æ»¤ä½è´¨é‡chunk
            if chunk_quality['overall'] < crawler.quality_threshold:
                continue
            
            # ç”Ÿæˆå…ƒæ•°æ®
            metadata = generate_metadata(chunk_text, pdf_path, title)
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            metadata['quality_score'] = round(chunk_quality['overall'], 4)
            metadata['chunk_index'] = i
            metadata['total_chunks'] = len(chunks)
            metadata['source_type'] = 'local_pdf'
            metadata['file_path'] = pdf_path
            
            upload_items.append({
                'file': chunk_text,
                'metadata': metadata
            })
        
        logging.info(f"âœ“ PDFå¤„ç†æˆåŠŸ: {pdf_path} | è´¨é‡: {quality_scores['overall']:.2f} | "
                    f"Chunks: {len(upload_items)}/{len(chunks)}")
        
        return upload_items
        
    except Exception as e:
        logging.error(f"PDFå¤„ç†å¤±è´¥ {pdf_path}: {e}")
        return []


def process_pdf_directory(pdf_dir: str, crawler: EnhancedCrawler) -> List[Dict]:
    """
    æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
    
    Args:
        pdf_dir: PDFæ–‡ä»¶ç›®å½•
        crawler: çˆ¬è™«å®ä¾‹
        
    Returns:
        æ‰€æœ‰chunkçš„åˆ—è¡¨
    """
    all_chunks = []
    
    if not os.path.exists(pdf_dir):
        logging.warning(f"ç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
        return all_chunks
    
    # è·å–æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = [f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]
    
    logging.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
    
    for i, pdf_file in enumerate(pdf_files, 1):
        logging.info(f"\nå¤„ç†PDFè¿›åº¦: {i}/{len(pdf_files)}")
        pdf_path = os.path.join(pdf_dir, pdf_file)
        chunks = process_pdf_file(pdf_path, crawler)
        all_chunks.extend(chunks)
    
    return all_chunks


def run_enhanced_crawler_demo(test_mode: bool = True):
    """
    è¿è¡Œå¢å¼ºç‰ˆçˆ¬è™«æ¼”ç¤º
    
    Args:
        test_mode: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬å–å°‘é‡é¡µé¢ï¼‰
    """
    logging.info("=" * 80)
    logging.info("å¢å¼ºç‰ˆç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»ºç³»ç»Ÿ")
    logging.info("=" * 80)
    
    # ç¬¬ä¸€æ­¥ï¼šæ•°æ®åº“,ä½¿ç”¨finalä½œä¸ºæœ€ç»ˆç‰ˆæœ¬
    logging.info("\nã€æ­¥éª¤1ã€‘è®¾ç½®å‘é‡æ•°æ®åº“")
    #æœ€ç»ˆåˆ›å»ºçš„æ•°æ®åº“ä¸ºï¼šstudent_Group12_finalï¼Œä½¿ç”¨cosineç›¸ä¼¼åº¦
    db_name = "student_Group12_final"
    metric = "cosine"
    logging.info(f"âœ“ æ•°æ®åº“è®¾ç½®æˆåŠŸ: {db_name} (metric={metric})")
    
    # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–çˆ¬è™«
    logging.info("\nã€æ­¥éª¤2ã€‘åˆå§‹åŒ–å¢å¼ºç‰ˆçˆ¬è™«")
    crawler = EnhancedCrawler(
        chunk_size=1500,
        chunk_overlap=150,
        min_chunk_size=100,
        quality_threshold=0.4, 
        aggressive_cleaning=False
    )
    
    # ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡URLåˆ—è¡¨
    logging.info("\nã€æ­¥éª¤3ã€‘å‡†å¤‡æ•°æ®æº")
    url_groups = get_cybersecurity_urls()
    
    # æµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªç±»åˆ«åªå–å°‘é‡URL
    if test_mode:
        logging.info("âš ï¸ æµ‹è¯•æ¨¡å¼ï¼šæ¯ä¸ªç±»åˆ«åªçˆ¬å–å‰2ä¸ªURL")
        test_urls = []
        for category, urls in url_groups.items():
            test_urls.extend(urls[:2])  # æ¯ä¸ªç±»åˆ«å–2ä¸ª
            logging.info(f"  - {category}: {min(2, len(urls))} ä¸ªURL")
        urls_to_crawl = test_urls
    else:
        # å®Œæ•´æ¨¡å¼ï¼šçˆ¬å–æ‰€æœ‰URL
        urls_to_crawl = []
        for category, urls in url_groups.items():
            urls_to_crawl.extend(urls)
            logging.info(f"  - {category}: {len(urls)} ä¸ªURL")
    
    logging.info(f"\næ€»è®¡å‡†å¤‡çˆ¬å–: {len(urls_to_crawl)} ä¸ªURL")
    
    # ç¬¬å››æ­¥ï¼šçˆ¬å–ç½‘é¡µ
    logging.info("\nã€æ­¥éª¤4ã€‘å¼€å§‹çˆ¬å–ç½‘é¡µ")
    all_chunks = crawler.process_urls(urls_to_crawl, delay=1.0)
    
    # ç¬¬äº”æ­¥ï¼šå¤„ç†æœ¬åœ°PDFï¼ˆå¦‚æœæœ‰ï¼‰
    logging.info("\nã€æ­¥éª¤5ã€‘å¤„ç†æœ¬åœ°PDFæ–‡ä»¶")
    pdf_dir = os.path.join(os.path.dirname(__file__), "pdf_documents")
    
    if os.path.exists(pdf_dir):
        pdf_chunks = process_pdf_directory(pdf_dir, crawler)
        all_chunks.extend(pdf_chunks)
        logging.info(f"âœ“ PDFå¤„ç†å®Œæˆï¼Œæ–°å¢ {len(pdf_chunks)} ä¸ªchunk")
    else:
        logging.info(f"â„¹ï¸ PDFç›®å½•ä¸å­˜åœ¨: {pdf_dir}")
        logging.info(f"   å¦‚éœ€å¤„ç†PDFï¼Œè¯·åˆ›å»ºè¯¥ç›®å½•å¹¶æ”¾å…¥PDFæ–‡ä»¶")
    
    # ç¬¬å…­æ­¥ï¼šæ‰“å°ç»Ÿè®¡
    crawler.print_stats()
    
    # ç¬¬ä¸ƒæ­¥ï¼šä¸Šä¼ åˆ°æ•°æ®åº“
    if all_chunks:
        logging.info("\nã€æ­¥éª¤6ã€‘ä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“")
        file_ids = upload_chunks(db_name, all_chunks)
        logging.info(f"âœ“ ä¸Šä¼ å®Œæˆï¼Œå…± {len(file_ids)} ä¸ªæ–‡æœ¬å—")
    else:
        logging.warning("æ²¡æœ‰å¯ä¸Šä¼ çš„å†…å®¹ï¼")
        return None, []
    
    # ç¬¬å…«æ­¥ï¼šæµ‹è¯•æœç´¢
    logging.info("\nã€æ­¥éª¤7ã€‘æµ‹è¯•æœç´¢åŠŸèƒ½")
    test_queries = [
        "SQLæ³¨å…¥æ”»å‡»åŸç†å’Œé˜²å¾¡æ–¹æ³•",
        "è·¨ç«™è„šæœ¬XSSæ”»å‡»",
        "phishing attack indicators",
        "å¦‚ä½•è¿›è¡Œæ¸—é€æµ‹è¯•",
    ]
    
    for query in test_queries[:2]:  # åªæµ‹è¯•å‰2ä¸ª
        logging.info(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
        payload = {
            "token": TOKEN,
            "query": query,
            "top_k": 3,
            "metric_type": metric,
            "score_threshold": 0.0
        }
        
        try:
            resp = requests.post(f"{BASE_URL}/databases/{db_name}/search", json=payload)
            if resp.status_code == 200:
                results = resp.json().get("files", [])
                logging.info(f"  è¿”å› {len(results)} ä¸ªç»“æœ")
                for i, r in enumerate(results[:2], 1):
                    score = r.get("score", 0)
                    quality = r.get("metadata", {}).get("quality_score", "N/A")
                    source_type = r.get("metadata", {}).get("source_type", "N/A")
                    preview = r.get("text", "")[:80]
                    logging.info(f"  [{i}] ç›¸ä¼¼åº¦: {score:.4f} | è´¨é‡: {quality} | ç±»å‹: {source_type}")
                    logging.info(f"      é¢„è§ˆ: {preview}...")
            else:
                logging.error(f"  æœç´¢å¤±è´¥: {resp.status_code}")
        except Exception as e:
            logging.error(f"  æœç´¢å¤±è´¥: {e}")
    
    logging.info("\n" + "=" * 80)
    logging.info("ğŸ‰ å¢å¼ºç‰ˆçˆ¬è™«æ‰§è¡Œå®Œæˆï¼")
    logging.info(f"ğŸ“Š æ•°æ®åº“åç§°: {db_name}")
    logging.info(f"ğŸ“Š æœ€ç»ˆchunkæ•°: {len(file_ids)}")
    logging.info("=" * 80)
    
    return db_name, file_ids


if __name__ == "__main__":
    try:
        # è¿è¡Œæ¼”ç¤ºï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
        db_name, file_ids = run_enhanced_crawler_demo(test_mode=False)
        
        if db_name:
            print("\n" + "=" * 80)
            print("âœ… ç³»ç»Ÿæ‰§è¡ŒæˆåŠŸï¼")
            print(f"æ•°æ®åº“åç§°: {db_name}")
            print(f"ä¸Šä¼ æ–‡ä»¶æ•°: {len(file_ids)}")
            print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
            print(f"1. åœ¨åº”ç”¨ä¸­ä½¿ç”¨æ•°æ®åº“å: '{db_name}'")
            print("=" * 80)
        else:
            print("\nâŒ ç³»ç»Ÿæ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            
    except KeyboardInterrupt:
        logging.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logging.error(f"\nâŒ ç³»ç»Ÿæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise

