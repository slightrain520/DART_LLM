#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
cybersec_rag_pipeline.py - ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»ºPipeline

åŠŸèƒ½ï¼š
- ç¨‹åºåŒ–æŠ“å–ç½‘é¡µ/PDFï¼ˆç¤ºä¾‹æ”¯æŒ MITRE ATT&CKã€ä»»æ„ç½‘é¡µã€ç›´æ¥ PDF é“¾æ¥ï¼‰
- æå–ä¸æ¸…æ´—æ­£æ–‡
- åˆ†æ®µï¼ˆchunkï¼‰ä¸é‡å 
- è‡ªåŠ¨ç”Ÿæˆå…ƒæ•°æ®ï¼ˆsource, url, title, date, tags (CVE/CWE/T\d+)ï¼‰
- æ‰¹é‡ä¸Šä¼ åˆ°åç«¯æ•°æ®åº“ï¼ˆä½¿ç”¨ä½ çš„ test_api_20251015.py é£æ ¼æ¥å£ï¼‰
"""

# ==================== æ ‡å‡†åº“å¯¼å…¥ ====================
import re                # æ­£åˆ™è¡¨è¾¾å¼åº“ï¼Œç”¨äºæ–‡æœ¬æ¨¡å¼åŒ¹é…ï¼ˆå¦‚æå–CVEç¼–å·ã€æ—¥æœŸç­‰ï¼‰
import os                # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œ
import time              # æ—¶é—´ç›¸å…³å‡½æ•°ï¼Œç”¨äºå»¶è¿Ÿå’Œæ—¶é—´æˆ³ç”Ÿæˆ
import json              # JSONæ•°æ®å¤„ç†
import hashlib           # å“ˆå¸Œç®—æ³•åº“ï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶å”¯ä¸€æ ‡è¯†
import logging           # æ—¥å¿—è®°å½•åº“ï¼Œç”¨äºè¾“å‡ºç¨‹åºè¿è¡Œä¿¡æ¯
from typing import List, Dict, Tuple, Optional  # ç±»å‹æ³¨è§£ï¼Œå¢å¼ºä»£ç å¯è¯»æ€§
from urllib.parse import urlparse, urljoin      # URLè§£æå·¥å…·ï¼Œç”¨äºå¤„ç†ç›¸å¯¹/ç»å¯¹è·¯å¾„

# ==================== ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ====================
import requests          # HTTPè¯·æ±‚åº“ï¼Œç”¨äºå‘é€ç½‘ç»œè¯·æ±‚ä¸‹è½½ç½‘é¡µå’Œæ–‡ä»¶
from bs4 import BeautifulSoup  # HTML/XMLè§£æåº“ï¼Œç”¨äºä»ç½‘é¡µä¸­æå–æ–‡æœ¬å†…å®¹

# PDFæ–‡æœ¬æå–å™¨ï¼ˆpdfminer.sixåº“ï¼‰- ç”¨äºä»PDFæ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬
from pdfminer.high_level import extract_text

# ==================== é…ç½®åŒºï¼ˆä¿®æ”¹ä¸ºä½ çš„ç¯å¢ƒï¼‰ ====================
BASE_URL = "http://10.1.0.220:9002/api"   # åç«¯APIåœ°å€ï¼Œä¸test_api_20251015.pyä¿æŒä¸€è‡´
TOKEN = "e-1qa4tLR9N_AnEEBemwaiOBoyoRoFHr00W0Wb3Uk5tWE5ziWJiCHh7sM1b73T2s"  # ä½ çš„è®¤è¯Token
# å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ–¹å¼ï¼šL2(æ¬§æ°è·ç¦») æˆ– cosine(ä½™å¼¦ç›¸ä¼¼åº¦) æˆ– IP(å†…ç§¯)
METRIC_TYPE = "cosine"

# æ‰¹é‡ä¸Šä¼ æ—¶æ¯æ‰¹æ¬¡çš„æ–‡ä»¶æ•°é‡ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
UPLOAD_BATCH = 10

# PDFæ–‡ä»¶ä¸´æ—¶ä¸‹è½½ç›®å½•ï¼ˆWindowsç³»ç»Ÿä½¿ç”¨ä¸´æ—¶ç›®å½•ï¼‰
PDF_TMP_DIR = os.path.join(os.getenv("TEMP", "."), "pdf_cache")
os.makedirs(PDF_TMP_DIR, exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º

# æ—¥å¿—é…ç½®ï¼šè®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºINFOï¼Œæ ¼å¼åŒ…å«æ—¶é—´ã€çº§åˆ«å’Œæ¶ˆæ¯
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

# ==================== å·¥å…·å‡½æ•° ====================

def safe_request_get(url, **kwargs):
    """
    å®‰å…¨çš„HTTP GETè¯·æ±‚å°è£…å‡½æ•°
    
    å‚æ•°:
        url: è¦è¯·æ±‚çš„ç½‘å€
        **kwargs: å…¶ä»–requests.get()æ”¯æŒçš„å‚æ•°
    
    è¿”å›:
        requests.Responseå¯¹è±¡ï¼Œå¤±è´¥è¿”å›None
    
    åŠŸèƒ½è¯´æ˜:
        1. è‡ªåŠ¨æ·»åŠ User-Agentå¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼ˆé¿å…è¢«åçˆ¬è™«æ‹¦æˆªï¼‰
        2. è®¾ç½®20ç§’è¶…æ—¶æ—¶é—´ï¼ˆé¿å…é•¿æ—¶é—´ç­‰å¾…ï¼‰
        3. è‡ªåŠ¨æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œ4xx/5xxä¼šæŠ›å‡ºå¼‚å¸¸
        4. æ•è·æ‰€æœ‰å¼‚å¸¸å¹¶è®°å½•æ—¥å¿—
    """
    # ä»kwargsä¸­å–å‡ºheaderså‚æ•°ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºç©ºå­—å…¸
    headers = kwargs.pop("headers", {})
    # å¦‚æœheadersä¸­æ²¡æœ‰User-Agentï¼Œåˆ™è®¾ç½®é»˜è®¤å€¼ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼‰
    headers.setdefault("User-Agent", "Mozilla/5.0 (compatible; CyberSecRAG/1.0)")
    
    try:
        # å‘é€GETè¯·æ±‚ï¼šheadersæŒ‡å®šè¯·æ±‚å¤´ï¼Œtimeoutè®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œ**kwargsä¼ é€’å…¶ä»–å‚æ•°
        r = requests.get(url, headers=headers, timeout=20, **kwargs)
        # æ£€æŸ¥å“åº”çŠ¶æ€ç ï¼Œå¦‚æœæ˜¯4xxæˆ–5xxä¼šæŠ›å‡ºHTTPErrorå¼‚å¸¸
        r.raise_for_status()
        return r
    except Exception as e:
        # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼ˆç½‘ç»œé”™è¯¯ã€è¶…æ—¶ã€HTTPé”™è¯¯ç­‰ï¼‰ï¼Œè®°å½•è­¦å‘Šæ—¥å¿—
        logging.warning(f"GET {url} å¤±è´¥: {e}")
        return None

def download_pdf_to_text(url: str, local_tmp: str = None) -> str:
    """
    ä¸‹è½½PDFæ–‡ä»¶å¹¶æå–å…¶ä¸­çš„æ–‡æœ¬å†…å®¹
    
    å‚æ•°:
        url: PDFæ–‡ä»¶çš„URLåœ°å€
        local_tmp: ä¸´æ—¶æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼ŒNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®çš„PDF_TMP_DIR
    
    è¿”å›:
        æå–çš„æ–‡æœ¬å†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    
    å·¥ä½œæµç¨‹:
        1. ä¸‹è½½PDFæ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶ç›®å½•
        2. ä½¿ç”¨pdfminer.sixåº“æå–æ–‡æœ¬
        3. è¿”å›æå–çš„æ–‡æœ¬å†…å®¹
    """
    if local_tmp is None:
        local_tmp = PDF_TMP_DIR  # ä½¿ç”¨é…ç½®çš„ä¸´æ—¶ç›®å½•
    
    logging.info(f"ä¸‹è½½å¹¶æå– PDF: {url}")
    
    # stream=True: æµå¼ä¸‹è½½ï¼Œé€‚åˆå¤§æ–‡ä»¶ï¼Œä¸ä¼šä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
    r = safe_request_get(url, stream=True)
    if not r:
        return ""
    
    # è§£æURLï¼Œæå–ä¿¡æ¯ï¼ˆè¿™é‡Œä¸»è¦ç”¨äºæ—¥å¿—è®°å½•ï¼‰
    parsed = urlparse(url)
    
    # ä½¿ç”¨SHA1å“ˆå¸Œç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼Œé¿å…æ–‡ä»¶åå†²çª
    # encode()å°†å­—ç¬¦ä¸²è½¬ä¸ºå­—èŠ‚ï¼Œhexdigest()å¾—åˆ°16è¿›åˆ¶å“ˆå¸Œå€¼
    fn = os.path.join(local_tmp, hashlib.sha1(url.encode()).hexdigest() + ".pdf")
    
    # ä»¥äºŒè¿›åˆ¶å†™å…¥æ¨¡å¼æ‰“å¼€æ–‡ä»¶
    with open(fn, "wb") as f:
        # iter_content(): è¿­ä»£å“åº”å†…å®¹ï¼Œæ¯æ¬¡è¯»å–16KB (1024*16å­—èŠ‚)
        for chunk in r.iter_content(1024*16):
            if chunk:  # è¿‡æ»¤æ‰ä¿æŒè¿æ¥çš„ç©ºchunk
                f.write(chunk)
    
    try:
        # ä½¿ç”¨pdfminer.sixåº“æå–PDFä¸­çš„æ–‡æœ¬
        text = extract_text(fn)
        
        # æå–æˆåŠŸååˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
        try:
            os.remove(fn)
        except:
            pass  # åˆ é™¤å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            
        return text
    except Exception as e:
        # PDFå¯èƒ½æŸåã€åŠ å¯†æˆ–æ ¼å¼ä¸æ”¯æŒ
        logging.warning(f"pdfminer æŠ½å–å¤±è´¥: {e}")
        return ""

def html_to_text(html: str) -> Tuple[str, str]:
    """
    ä»HTMLç½‘é¡µä¸­æå–æ ‡é¢˜å’Œæ­£æ–‡å†…å®¹ï¼Œè‡ªåŠ¨è¿‡æ»¤å¯¼èˆªæ ã€å¹¿å‘Šç­‰å™ªå£°
    
    å‚æ•°:
        html: HTMLæºä»£ç å­—ç¬¦ä¸²
    
    è¿”å›:
        (title, text): æ ‡é¢˜å’Œæ­£æ–‡çš„å…ƒç»„
    
    æ¸…æ´—ç­–ç•¥:
        1. æå–ç½‘é¡µæ ‡é¢˜
        2. ç§»é™¤è„šæœ¬ã€æ ·å¼ã€å¯¼èˆªã€é¡µè„šç­‰éæ­£æ–‡å…ƒç´ 
        3. ç§»é™¤å¹¿å‘Šã€é¢åŒ…å±‘ç­‰å™ªå£°å†…å®¹
        4. æ™ºèƒ½å®šä½ä¸»è¦å†…å®¹åŒºåŸŸï¼ˆ<main>ã€<article>æˆ–æ–‡æœ¬æœ€å¤šçš„<div>ï¼‰
        5. æ¸…ç†å¤šä½™ç©ºç™½å­—ç¬¦
    """
    # BeautifulSoupè§£æHTMLï¼Œhtml.parseræ˜¯Pythonå†…ç½®çš„è§£æå™¨
    soup = BeautifulSoup(html, "html.parser")
    
    # æå–ç½‘é¡µæ ‡é¢˜ï¼ˆ<title>æ ‡ç­¾å†…å®¹ï¼‰
    title = (soup.title.string.strip() if soup.title and soup.title.string else "") or ""
    
    # ç§»é™¤æ— ç”¨æ ‡ç­¾ï¼šè„šæœ¬ã€æ ·å¼ã€å¯¼èˆªæ ã€é¡µçœ‰ã€é¡µè„šã€ä¾§è¾¹æ ã€è¡¨å•
    # soup()ç­‰åŒäºsoup.find_all()ï¼Œè¿”å›æ‰€æœ‰åŒ¹é…çš„æ ‡ç­¾
    for s in soup(["script", "style", "nav", "header", "footer", "aside", "form"]):
        s.decompose()  # decompose()ä»DOMæ ‘ä¸­å®Œå…¨åˆ é™¤è¯¥å…ƒç´ 
    
    # é€šè¿‡CSSé€‰æ‹©å™¨ç§»é™¤å¸¸è§çš„å™ªå£°å…ƒç´ 
    # . è¡¨ç¤ºclassé€‰æ‹©å™¨ï¼Œ# è¡¨ç¤ºidé€‰æ‹©å™¨
    noise_selectors = ['.breadcrumb', '.nav', '.footer', '#footer', '.sidebar', 
                       '.ad', '.advert', '.cookie', '.menu', '.banner']
    for sel in noise_selectors:
        # select()ä½¿ç”¨CSSé€‰æ‹©å™¨æŸ¥æ‰¾å…ƒç´ 
        for node in soup.select(sel):
            node.decompose()
    
    # æ™ºèƒ½å®šä½ä¸»è¦å†…å®¹åŒºåŸŸ
    # ä¼˜å…ˆæŸ¥æ‰¾è¯­ä¹‰åŒ–æ ‡ç­¾ <main> æˆ– <article>
    main = soup.find("main") or soup.find("article")
    
    if not main:
        # å¦‚æœæ²¡æœ‰è¯­ä¹‰åŒ–æ ‡ç­¾ï¼Œæ‰¾æ–‡æœ¬å†…å®¹æœ€å¤šçš„<div>æˆ–<section>
        candidates = soup.find_all(["div", "section"], recursive=True)
        if candidates:
            # max()æ‰¾å‡ºæ–‡æœ¬é•¿åº¦æœ€å¤§çš„å…ƒç´ 
            # keyå‚æ•°æŒ‡å®šæ¯”è¾ƒå‡½æ•°ï¼šè·å–å…ƒç´ çš„æ–‡æœ¬å¹¶è®¡ç®—é•¿åº¦
            main = max(candidates, key=lambda d: len(d.get_text(separator=" ", strip=True)) if d else 0)
        else:
            main = soup.body  # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨æ•´ä¸ªbody
    
    # æå–æ–‡æœ¬ï¼šseparator="\n"åœ¨æ ‡ç­¾é—´æ’å…¥æ¢è¡Œï¼Œstrip=Trueå»é™¤é¦–å°¾ç©ºç™½
    text = main.get_text(separator="\n", strip=True) if main else soup.get_text(separator="\n", strip=True)
    
    # æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†æ–‡æœ¬æ ¼å¼
    # \n{2,} åŒ¹é…2ä¸ªæˆ–å¤šä¸ªè¿ç»­æ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸º2ä¸ªæ¢è¡Œï¼ˆç»Ÿä¸€æ®µè½é—´è·ï¼‰
    text = re.sub(r'\n{2,}', '\n\n', text)
    # [ \t]{2,} åŒ¹é…2ä¸ªæˆ–å¤šä¸ªè¿ç»­ç©ºæ ¼/åˆ¶è¡¨ç¬¦ï¼Œæ›¿æ¢ä¸º1ä¸ªç©ºæ ¼
    text = re.sub(r'[ \t]{2,}', ' ', text)
    
    return title, text

# ==================== æ–‡æœ¬åˆ†å‰²é€»è¾‘ ====================

def split_text_into_chunks(text: str,
                           chunk_size_chars: int = 2000,
                           chunk_overlap_chars: int = 200,
                           separators: List[str] = None) -> List[str]:
    """
    æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å‡½æ•°ï¼šå°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunkï¼Œç”¨äºå‘é‡åŒ–å­˜å‚¨
    
    å‚æ•°:
        text: å¾…åˆ†å‰²çš„åŸå§‹æ–‡æœ¬
        chunk_size_chars: æ¯ä¸ªchunkçš„ç›®æ ‡å­—ç¬¦æ•°ï¼ˆå»ºè®®800-4000ï¼Œå–å†³äºembeddingæ¨¡å‹ï¼‰
        chunk_overlap_chars: ç›¸é‚»chunkä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼‰
        separators: åˆ†å‰²ç¬¦ä¼˜å…ˆçº§åˆ—è¡¨ï¼ˆä»å·¦åˆ°å³ä¼˜å…ˆçº§é™ä½ï¼‰
    
    è¿”å›:
        åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
    
    åˆ†å‰²ç­–ç•¥:
        1. ä¼˜å…ˆæŒ‰æ®µè½ï¼ˆåŒæ¢è¡Œï¼‰åˆ†å‰²
        2. åˆå¹¶å°æ®µè½åˆ°ç›®æ ‡å¤§å°
        3. è¶…é•¿æ®µè½æŒ‰å›ºå®šå¤§å°åˆ‡åˆ†
        4. æ·»åŠ chunké—´é‡å ï¼Œé¿å…ä¿¡æ¯åœ¨è¾¹ç•Œä¸¢å¤±
    
    ä¸ºä»€ä¹ˆéœ€è¦chunk_overlapï¼Ÿ
        - å¦‚æœæŸä¸ªæ¦‚å¿µæ­£å¥½è·¨è¶Šä¸¤ä¸ªchunkçš„è¾¹ç•Œï¼Œæ²¡æœ‰é‡å ä¼šå¯¼è‡´æ£€ç´¢æ—¶ä¿¡æ¯ä¸å®Œæ•´
        - é‡å éƒ¨åˆ†å……å½“"ç¼“å†²åŒº"ï¼Œç¡®ä¿é‡è¦ä¿¡æ¯ä¸ä¼šè¢«åˆ‡æ–­
    """
    if separators is None:
        # é»˜è®¤åˆ†éš”ç¬¦ä¼˜å…ˆçº§ï¼šæ®µè½ > è¡Œ > ä¸­æ–‡å¥å· > è‹±æ–‡å¥å· > é—®å· > æ„Ÿå¹å·
        separators = ["\n\n", "\n", "ã€‚", ".", "?", "!"]
    
    # ç¬¬ä¸€æ­¥ï¼šæŒ‰åŒæ¢è¡Œï¼ˆæ®µè½ï¼‰åˆ†å‰²æ–‡æœ¬
    # re.split()æ­£åˆ™åˆ†å‰²ï¼Œ\n{2,}åŒ¹é…2ä¸ªæˆ–æ›´å¤šæ¢è¡Œç¬¦
    paragraphs = [p.strip() for p in re.split(r'\n{2,}', text) if p.strip()]
    
    chunks = []  # æœ€ç»ˆçš„chunkåˆ—è¡¨
    cur = ""     # å½“å‰æ­£åœ¨æ„å»ºçš„chunk
    
    # ç¬¬äºŒæ­¥ï¼šå°†æ®µè½åˆå¹¶åˆ°æ¥è¿‘chunk_sizeçš„å¤§å°
    for p in paragraphs:
        # å¦‚æœåŠ å…¥å½“å‰æ®µè½åä¸è¶…è¿‡ç›®æ ‡å¤§å°ï¼Œåˆ™åˆå¹¶
        if len(cur) + len(p) + 1 <= chunk_size_chars:
            cur = (cur + "\n\n" + p).strip() if cur else p
        else:
            # å¦åˆ™ä¿å­˜å½“å‰chunkï¼Œå¼€å§‹æ–°chunk
            if cur:
                chunks.append(cur)
            
            # å¤„ç†è¶…é•¿æ®µè½ï¼šå¦‚æœå•ä¸ªæ®µè½è¶…è¿‡chunk_sizeï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
            if len(p) > chunk_size_chars:
                # ç®€å•ç­–ç•¥ï¼šæŒ‰å›ºå®šå®½åº¦æ»‘åŠ¨åˆ‡åˆ†
                # range(start, stop, step): ä»0å¼€å§‹ï¼Œæ¯æ¬¡æ­¥è¿›chunk_size_chars
                for i in range(0, len(p), chunk_size_chars):
                    chunks.append(p[i:i+chunk_size_chars])
                cur = ""
            else:
                cur = p  # å¼€å§‹æ–°chunk
    
    # æ·»åŠ æœ€åä¸€ä¸ªchunk
    if cur:
        chunks.append(cur)
    
    # ç¬¬ä¸‰æ­¥ï¼šæ·»åŠ chunké—´é‡å ï¼ˆæå‡æ£€ç´¢æ•ˆæœï¼‰
    if chunk_overlap_chars > 0:
        overlapped = []
        for i, c in enumerate(chunks):
            if i == 0:
                # ç¬¬ä¸€ä¸ªchunkä¿æŒåŸæ ·
                overlapped.append(c)
            else:
                # ä»å‰ä¸€ä¸ªchunkæœ«å°¾å–overlap_charsé•¿åº¦çš„æ–‡æœ¬
                prev = overlapped[-1]
                overlap = prev[-chunk_overlap_chars:] if len(prev) > chunk_overlap_chars else prev
                # å°†é‡å éƒ¨åˆ†æ‹¼æ¥åˆ°å½“å‰chunkå‰é¢
                merged = overlap + "\n\n" + c
                overlapped.append(merged)
        chunks = overlapped
    
    return chunks

# ==================== å…ƒæ•°æ®è‡ªåŠ¨æŠ½å– ====================

# ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆç¼–è¯‘åå¯é‡å¤ä½¿ç”¨ï¼Œæé«˜æ•ˆç‡ï¼‰
# \b è¡¨ç¤ºå•è¯è¾¹ç•Œï¼Œç¡®ä¿ç²¾ç¡®åŒ¹é…
CVE_RE = re.compile(r'\bCVE-\d{4}-\d{4,7}\b', re.IGNORECASE)  # CVEç¼–å·ï¼šCVE-2021-12345
MITRE_ATTACK_RE = re.compile(r'\bT\d{4}\b')                   # MITRE ATT&CKæŠ€æœ¯IDï¼šT1566
CWE_RE = re.compile(r'\bCWE-\d{1,5}\b', re.IGNORECASE)        # CWEç¼–å·ï¼šCWE-79
DATE_RE = re.compile(r'\b(20\d{2}[-/]\d{1,2}[-/]\d{1,2}|20\d{2})\b')  # æ—¥æœŸï¼š2021-01-15 æˆ– 2021

def generate_metadata(text: str, source_url: str, title: str = "") -> Dict:
    """
    ä»æ–‡æœ¬ä¸­è‡ªåŠ¨æå–å…ƒæ•°æ®æ ‡ç­¾ï¼Œç”¨äºåç»­çš„è¿‡æ»¤æ£€ç´¢
    
    å‚æ•°:
        text: æ–‡æœ¬å†…å®¹
        source_url: æ¥æºURL
        title: æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        åŒ…å«å…ƒæ•°æ®çš„å­—å…¸
    
    æå–å†…å®¹:
        - CVEç¼–å·: æ¼æ´æ ‡è¯†ç¬¦
        - MITRE ATT&CKæŠ€æœ¯ID: æ”»å‡»æŠ€æœ¯åˆ†ç±»
        - CWEç¼–å·: é€šç”¨å¼±ç‚¹æšä¸¾
        - æ—¥æœŸ: å‘å¸ƒæˆ–æ›´æ–°æ—¶é—´
        - åˆ†ç±»æ ‡ç­¾: æ ¹æ®å…³é”®è¯å¯å‘å¼åˆ¤æ–­ï¼ˆSQLæ³¨å…¥ã€é’“é±¼ã€å‹’ç´¢è½¯ä»¶ç­‰ï¼‰
    """
    metadata = {}
    metadata['source_url'] = source_url
    # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œå–æ–‡æœ¬å‰120å­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼ˆå»é™¤æ¢è¡Œï¼‰
    metadata['title'] = title or (text[:120].replace("\n", " ") if text else "")
    
    # æå–CVEç¼–å·ï¼ˆå¸¸è§æ¼æ´ä¸æš´éœ²ï¼‰
    # finditer()è¿”å›æ‰€æœ‰åŒ¹é…çš„è¿­ä»£å™¨ï¼Œgroup(0)è·å–å®Œæ•´åŒ¹é…
    cves = list(set([m.group(0).upper() for m in CVE_RE.finditer(text)]))
    if cves:
        metadata['cves'] = cves
    
    # æå–MITRE ATT&CKæŠ€æœ¯ID
    t_ids = list(set([m.group(0).upper() for m in MITRE_ATTACK_RE.finditer(text)]))
    if t_ids:
        metadata['mitre_techniques'] = t_ids
    
    # æå–CWEç¼–å·
    cwes = list(set([m.group(0).upper() for m in CWE_RE.finditer(text)]))
    if cwes:
        metadata['cwes'] = cwes
    
    # æå–æ—¥æœŸ/å¹´ä»½
    years = list(set([m.group(0) for m in DATE_RE.finditer(text)]))
    if years:
        metadata['dates'] = years
    
    # åŸºäºå…³é”®è¯çš„å¯å‘å¼åˆ†ç±»
    # re.search()åœ¨æ•´ä¸ªå­—ç¬¦ä¸²ä¸­æœç´¢åŒ¹é…ï¼Œre.Iè¡¨ç¤ºå¿½ç•¥å¤§å°å†™
    cats = []
    
    if re.search(r'\bsql injection|sqlæ³¨å…¥|sqlmap|sqlç›²æ³¨|unionæ³¨å…¥\b', text, re.I):
        cats.append('sql_injection')
    
    if re.search(r'\bxss|cross.?site.?script|è·¨ç«™è„šæœ¬\b', text, re.I):
        cats.append('xss')
    
    if re.search(r'\bphish(ing)?|é’“é±¼|ç½‘ç»œé’“é±¼\b', text, re.I):
        cats.append('phishing')
    
    if re.search(r'\bransomware|å‹’ç´¢è½¯ä»¶|å‹’ç´¢ç—…æ¯’\b', text, re.I):
        cats.append('ransomware')
    
    if re.search(r'\brce|remote code execution|è¿œç¨‹ä»£ç æ‰§è¡Œ\b', text, re.I):
        cats.append('rce')
    
    if re.search(r'\bddos|denial.?of.?service|æ‹’ç»æœåŠ¡\b', text, re.I):
        cats.append('ddos')
    
    if re.search(r'\bpenetration.?test|æ¸—é€æµ‹è¯•|pentest\b', text, re.I):
        cats.append('penetration_testing')
    
    if re.search(r'\bmalware|æ¶æ„è½¯ä»¶|æœ¨é©¬|trojan\b', text, re.I):
        cats.append('malware')
    
    # å¦‚æœæ–‡æœ¬ä¸­åŒ…å«CVEç¼–å·ï¼Œæ ‡è®°ä¸ºæ¼æ´ç±»
    if cves:
        cats.append('vulnerability')
    
    # å¦‚æœåŒ…å«MITREæŠ€æœ¯IDï¼Œæ ‡è®°ä¸ºæ”»å‡»æŠ€æœ¯ç±»
    if t_ids:
        cats.append('attack_technique')
    
    # å»é‡å¹¶ä¿å­˜
    metadata['categories'] = list(set(cats))
    
    return metadata

# ==================== åç«¯æ•°æ®åº“APIäº¤äº’ ====================

def create_database(metric_type: str = METRIC_TYPE) -> Tuple[str, str]:
    """
    è°ƒç”¨åç«¯APIåˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
    
    å‚æ•°:
        metric_type: å‘é‡ç›¸ä¼¼åº¦åº¦é‡æ–¹å¼ (L2/cosine/IP)
    
    è¿”å›:
        (æ•°æ®åº“åç§°, åº¦é‡ç±»å‹) å…ƒç»„
    
    APIè¯´æ˜:
        - ç«¯ç‚¹: POST /api/databases
        - å‚æ•°: database_name(æ•°æ®åº“å), token(è®¤è¯ä»¤ç‰Œ), metric_type(åº¦é‡æ–¹å¼)
        - æ•°æ®åº“å‘½åè§„åˆ™: student_{ç»„å}_{æ—¶é—´æˆ³}
    """
    db_name = f"student_Group12_final"
    logging.info(f"åˆ›å»ºæ•°æ®åº“: {db_name}")
    
    # å‘é€POSTè¯·æ±‚ï¼Œjsonå‚æ•°è‡ªåŠ¨å°†å­—å…¸åºåˆ—åŒ–ä¸ºJSONå¹¶è®¾ç½®Content-Type
    resp = requests.post(
        f"{BASE_URL}/databases", 
        json={
            "database_name": db_name, 
            "token": TOKEN, 
            "metric_type": metric_type
        }
    )
    
    # æ£€æŸ¥HTTPçŠ¶æ€ç ï¼Œé2xxä¼šæŠ›å‡ºå¼‚å¸¸
    resp.raise_for_status()
    logging.info("åˆ›å»ºæ•°æ®åº“å“åº”: %s", resp.json())
    
    return db_name, metric_type

def upload_chunks(db_name: str, chunks: List[Dict]) -> List[int]:
    """
    æ‰¹é‡ä¸Šä¼ æ–‡æœ¬chunkåˆ°æŒ‡å®šæ•°æ®åº“
    
    å‚æ•°:
        db_name: ç›®æ ‡æ•°æ®åº“åç§°
        chunks: chunkåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸ {'file': æ–‡æœ¬å†…å®¹, 'metadata': å…ƒæ•°æ®å­—å…¸}
    
    è¿”å›:
        ä¸Šä¼ æˆåŠŸçš„file_idåˆ—è¡¨
    
    åˆ†æ‰¹ä¸Šä¼ åŸå› :
        - é¿å…å•æ¬¡è¯·æ±‚ä½“ç§¯è¿‡å¤§å¯¼è‡´è¶…æ—¶
        - æé«˜ä¸Šä¼ ç¨³å®šæ€§ï¼Œéƒ¨åˆ†å¤±è´¥ä¸å½±å“æ•´ä½“
        - åç«¯å¯èƒ½æœ‰å•æ¬¡è¯·æ±‚å¤§å°é™åˆ¶
    """
    file_ids = []
    
    # åˆ†æ‰¹ä¸Šä¼ ï¼šrange(èµ·å§‹, ç»ˆæ­¢, æ­¥é•¿)
    # ä¾‹å¦‚ï¼šchunksæœ‰25ä¸ªï¼ŒUPLOAD_BATCH=10ï¼Œåˆ™åˆ†3æ‰¹ï¼š0-9, 10-19, 20-24
    for i in range(0, len(chunks), UPLOAD_BATCH):
        # åˆ‡ç‰‡è·å–å½“å‰æ‰¹æ¬¡ï¼šiåˆ°i+UPLOAD_BATCH
        batch = chunks[i:i+UPLOAD_BATCH]
        
        # æ„é€ è¯·æ±‚ä½“
        payload = {
            "files": batch,  # æ–‡ä»¶åˆ—è¡¨
            "token": TOKEN   # è®¤è¯ä»¤ç‰Œ
        }
        
        # POSTè¯·æ±‚ä¸Šä¼ æ–‡ä»¶
        # APIç«¯ç‚¹: POST /api/databases/{db_name}/files
        resp = requests.post(f"{BASE_URL}/databases/{db_name}/files", json=payload)
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if resp.status_code != 200:
            logging.error("ä¸Šä¼ æ‰¹æ¬¡å¤±è´¥: %s %s", resp.status_code, resp.text)
            raise RuntimeError("ä¸Šä¼ å¤±è´¥")
        
        # è§£æå“åº”JSON
        j = resp.json()
        logging.info("ä¸Šä¼ æ‰¹æ¬¡ %d-%d å®Œæˆ: %s", i, min(i+UPLOAD_BATCH, len(chunks))-1, j)
        
        # æ”¶é›†è¿”å›çš„file_id
        # extend()å°†åˆ—è¡¨ä¸­çš„æ‰€æœ‰å…ƒç´ æ·»åŠ åˆ°file_ids
        file_ids.extend(j.get("file_ids", []))
        
        # çŸ­æš‚å»¶è¿Ÿï¼Œç»™åç«¯æ—¶é—´å¤„ç†å’ŒæŒä¹…åŒ–æ•°æ®
        # é¿å…è¯·æ±‚è¿‡å¿«å¯¼è‡´æ•°æ®åº“æœªåŠæ—¶flush
        time.sleep(0.5)
    
    return file_ids

# ==================== ç½‘ç«™çˆ¬å–å‡½æ•°ï¼ˆæ”¯æŒå¤šä¸ªæ•°æ®æºï¼‰ ====================

def crawl_mitre_attack(base_index_url="https://attack.mitre.org/techniques/enterprise/", max_pages=50):
    """
    çˆ¬å–MITRE ATT&CKæ”»å‡»æŠ€æœ¯çŸ¥è¯†åº“
    
    æ•°æ®æºä»‹ç»:
        MITRE ATT&CKæ˜¯å…¨çƒæƒå¨çš„ç½‘ç»œæ”»å‡»è¡Œä¸ºçŸ¥è¯†åº“ï¼ŒåŒ…å«å„ç§æ”»å‡»æŠ€æœ¯ã€
        æˆ˜æœ¯å’Œç¨‹åºçš„è¯¦ç»†æè¿°ï¼Œè¢«å®‰å…¨è¡Œä¸šå¹¿æ³›é‡‡ç”¨ä½œä¸ºå¨èƒå»ºæ¨¡æ ‡å‡†ã€‚
    
    å‚æ•°:
        base_index_url: MITRE ATT&CKæŠ€æœ¯åˆ—è¡¨é¡µURL
        max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°ï¼ˆé¿å…è¿‡åº¦è¯·æ±‚ï¼‰
    
    è¿”å›:
        é¡µé¢ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« url, title, text, meta
    
    çˆ¬å–ç­–ç•¥:
        1. å…ˆè®¿é—®ç´¢å¼•é¡µï¼Œæå–æ‰€æœ‰æŠ€æœ¯é¡µé¢çš„é“¾æ¥
        2. é€ä¸ªè®¿é—®æŠ€æœ¯é¡µé¢ï¼Œæå–æ­£æ–‡
        3. è‡ªåŠ¨å»é‡ï¼Œè¿‡æ»¤ä½è´¨é‡å†…å®¹
        4. éµå®ˆçˆ¬è™«ç¤¼ä»ªï¼Œæ¯æ¬¡è¯·æ±‚é—´éš”0.6ç§’
    
    æ³¨æ„äº‹é¡¹:
        - è¯·éµå®ˆç½‘ç«™çš„robots.txtè§„åˆ™
        - ä¸è¦è®¾ç½®è¿‡é«˜çš„max_pagesï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
        - MITREé¡µé¢ç»“æ„å¯èƒ½å˜åŒ–ï¼Œéœ€è¦å®šæœŸæ£€æŸ¥CSSé€‰æ‹©å™¨
    """
    results = []
    logging.info(f"å¼€å§‹çˆ¬å– MITRE ATT&CK: {base_index_url}")
    
    # ç¬¬ä¸€æ­¥ï¼šè·å–ç´¢å¼•é¡µ
    idx_r = safe_request_get(base_index_url)
    if not idx_r:
        logging.warning("æ— æ³•è®¿é—®MITRE ATT&CKç´¢å¼•é¡µ")
        return results
    
    # è§£æHTML
    soup = BeautifulSoup(idx_r.text, "html.parser")
    
    # æå–æ‰€æœ‰æŠ€æœ¯é¡µé¢é“¾æ¥
    # CSSé€‰æ‹©å™¨: a[href^='/techniques/'] åŒ¹é…hrefä»¥/techniques/å¼€å¤´çš„<a>æ ‡ç­¾
    for a in soup.select("a[href^='/techniques/']"):
        href = a.get('href')
        if not href or '/techniques/' not in href:
            continue
        
        # urljoin()å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL
        # ä¾‹å¦‚: /techniques/T1566 -> https://attack.mitre.org/techniques/T1566
        full = urljoin(base_index_url, href)
        
        # å»é‡ï¼šæ£€æŸ¥URLæ˜¯å¦å·²å­˜åœ¨
        if full in [r['url'] for r in results]:
            continue
        
        results.append({"url": full, "title": a.get_text(strip=True)})
    
    logging.info(f"ä»ç´¢å¼•é¡µæå–åˆ° {len(results)} ä¸ªæŠ€æœ¯é¡µé¢é“¾æ¥")
    
    # ç¬¬äºŒæ­¥ï¼šé€ä¸ªè®¿é—®æŠ€æœ¯é¡µé¢ï¼Œæå–æ­£æ–‡
    pages = []
    for item in results[:max_pages]:  # é™åˆ¶çˆ¬å–æ•°é‡
        logging.info(f"æ­£åœ¨çˆ¬å–: {item['url']}")
        r = safe_request_get(item['url'])
        if not r:
            continue
        
        # æå–æ ‡é¢˜å’Œæ­£æ–‡
        title, text = html_to_text(r.text)
        
        # è¿‡æ»¤ä½è´¨é‡å†…å®¹ï¼šæ­£æ–‡å¤ªçŸ­çš„é¡µé¢å¯èƒ½æ˜¯é”™è¯¯é¡µæˆ–ç©ºé¡µ
        if len(text) < 200:
            logging.info("é¡µé¢æ­£æ–‡å¤ªçŸ­ï¼Œè·³è¿‡ %s", item['url'])
            continue
        
        # ç”Ÿæˆå…ƒæ•°æ®ï¼ˆè‡ªåŠ¨æå–CVEã€æŠ€æœ¯IDç­‰ï¼‰
        meta = generate_metadata(text, item['url'], title)
        pages.append({"url": item['url'], "title": title, "text": text, "meta": meta})
        
        # ç¤¼è²Œå»¶è¿Ÿï¼šé¿å…è¯·æ±‚è¿‡å¿«è¢«æœåŠ¡å™¨å°ç¦
        time.sleep(0.6)
    
    logging.info(f"æˆåŠŸçˆ¬å– {len(pages)} ä¸ªMITRE ATT&CKé¡µé¢")
    return pages

def crawl_generic_urls(urls: List[str]) -> List[Dict]:
    """
    é€šç”¨URLçˆ¬å–å‡½æ•°ï¼Œæ”¯æŒç½‘é¡µå’ŒPDFæ–‡ä»¶
    
    å‚æ•°:
        urls: URLåˆ—è¡¨ï¼Œå¯ä»¥æ˜¯HTMLç½‘é¡µæˆ–PDFæ–‡ä»¶é“¾æ¥
    
    è¿”å›:
        çˆ¬å–ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« url, title, text, meta
    
    åŠŸèƒ½ç‰¹ç‚¹:
        - è‡ªåŠ¨è¯†åˆ«PDFé“¾æ¥ï¼ˆé€šè¿‡.pdfåç¼€ï¼‰
        - PDFä½¿ç”¨pdfmineræå–ï¼Œç½‘é¡µä½¿ç”¨BeautifulSoupè§£æ
        - è‡ªåŠ¨è¿‡æ»¤ç©ºå†…å®¹å’Œä½è´¨é‡é¡µé¢
        - ä¸ºæ¯ä¸ªé¡µé¢è‡ªåŠ¨ç”Ÿæˆå…ƒæ•°æ®æ ‡ç­¾
    
    ä½¿ç”¨åœºæ™¯:
        - çˆ¬å–å®‰å…¨å‚å•†çš„æ¼æ´å…¬å‘Šé¡µé¢
        - ä¸‹è½½å®‰å…¨æŠ¥å‘ŠPDF
        - æ‰¹é‡çˆ¬å–æŠ€æœ¯åšå®¢æ–‡ç« 
    """
    collected = []
    
    for u in urls:
        logging.info("æ­£åœ¨å¤„ç†: %s", u)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºPDFæ–‡ä»¶ï¼ˆé€šè¿‡URLåç¼€ï¼‰
        if u.lower().endswith(".pdf"):
            # PDFå¤„ç†åˆ†æ”¯
            txt = download_pdf_to_text(u)
            if not txt:
                logging.warning(f"PDFæå–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º: {u}")
                continue
            
            # PDFé€šå¸¸æ²¡æœ‰HTMLæ ‡é¢˜ï¼Œæå–æ–‡ä»¶åä½œä¸ºæ ‡é¢˜
            pdf_title = u.split('/')[-1].replace('.pdf', '')
            meta = generate_metadata(txt, u, title=pdf_title)
            collected.append({"url": u, "title": pdf_title, "text": txt, "meta": meta})
        else:
            # HTMLç½‘é¡µå¤„ç†åˆ†æ”¯
            r = safe_request_get(u)
            if not r:
                continue
            
            # æå–æ ‡é¢˜å’Œæ­£æ–‡
            title, text = html_to_text(r.text)
            
            # è¿‡æ»¤ä½è´¨é‡å†…å®¹
            if not text or len(text) < 50:
                logging.info("æ­£æ–‡å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œè·³è¿‡: %s", u)
                continue
            
            # ç”Ÿæˆå…ƒæ•°æ®
            meta = generate_metadata(text, u, title)
            collected.append({"url": u, "title": title, "text": text, "meta": meta})
        
        # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(0.4)
    
    logging.info(f"æˆåŠŸå¤„ç† {len(collected)}/{len(urls)} ä¸ªURL")
    return collected


def crawl_cwe_top25() -> List[Dict]:
    """
    çˆ¬å–CWE Top 25æœ€å±é™©çš„è½¯ä»¶å¼±ç‚¹åˆ—è¡¨
    
    æ•°æ®æºä»‹ç»:
        CWE (Common Weakness Enumeration) æ˜¯ç”±MITREç»´æŠ¤çš„è½¯ä»¶å®‰å…¨å¼±ç‚¹åˆ†ç±»ç³»ç»Ÿï¼Œ
        Top 25åˆ—è¡¨å±•ç¤ºäº†æœ€å¸¸è§å’Œæœ€å±é™©çš„è½¯ä»¶å®‰å…¨å¼±ç‚¹ã€‚
    
    è¿”å›:
        å¼±ç‚¹æè¿°é¡µé¢åˆ—è¡¨
    """
    base_url = "https://cwe.mitre.org/top25/archive/2023/2023_top25_list.html"
    logging.info(f"å¼€å§‹çˆ¬å– CWE Top 25: {base_url}")
    
    pages = []
    r = safe_request_get(base_url)
    if not r:
        logging.warning("æ— æ³•è®¿é—®CWE Top 25é¡µé¢")
        return pages
    
    # æå–ä¸»é¡µå†…å®¹
    title, text = html_to_text(r.text)
    if text and len(text) > 200:
        meta = generate_metadata(text, base_url, title)
        pages.append({"url": base_url, "title": title, "text": text, "meta": meta})
    
    logging.info(f"æˆåŠŸçˆ¬å–CWE Top 25ä¸»é¡µ")
    return pages


def crawl_owasp_top10() -> List[Dict]:
    """
    çˆ¬å–OWASP Top 10 Webåº”ç”¨å®‰å…¨é£é™©
    
    æ•°æ®æºä»‹ç»:
        OWASP (Open Web Application Security Project) Top 10æ˜¯Webåº”ç”¨å®‰å…¨é¢†åŸŸ
        æœ€æƒå¨çš„é£é™©åˆ—è¡¨ï¼Œæ¶µç›–äº†æœ€å…³é”®çš„Webå®‰å…¨å¨èƒã€‚
    
    è¿”å›:
        é£é™©æè¿°é¡µé¢åˆ—è¡¨
    """
    # OWASP Top 10 2021ç‰ˆæœ¬
    urls = [
        "https://owasp.org/Top10/",
        "https://owasp.org/Top10/A01_2021-Broken_Access_Control/",
        "https://owasp.org/Top10/A02_2021-Cryptographic_Failures/",
        "https://owasp.org/Top10/A03_2021-Injection/",
        "https://owasp.org/Top10/A04_2021-Insecure_Design/",
        "https://owasp.org/Top10/A05_2021-Security_Misconfiguration/",
        "https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/",
        "https://owasp.org/Top10/A07_2021-Identification_and_Authentication_Failures/",
        "https://owasp.org/Top10/A08_2021-Software_and_Data_Integrity_Failures/",
        "https://owasp.org/Top10/A09_2021-Security_Logging_and_Monitoring_Failures/",
        "https://owasp.org/Top10/A10_2021-Server-Side_Request_Forgery_%28SSRF%29/",
    ]
    
    logging.info("å¼€å§‹çˆ¬å– OWASP Top 10")
    return crawl_generic_urls(urls)

# ==================== ä¸»æµç¨‹Pipeline ====================

def pipeline_demo():
    """
    å®Œæ•´çš„çŸ¥è¯†åº“æ„å»ºPipelineæ¼”ç¤º
    
    æ‰§è¡Œæµç¨‹:
        1. åˆ›å»ºå‘é‡æ•°æ®åº“
        2. ä»å¤šä¸ªæ•°æ®æºçˆ¬å–å†…å®¹
        3. æ–‡æœ¬åˆ†å‰²å’Œå…ƒæ•°æ®æå–
        4. æ‰¹é‡ä¸Šä¼ åˆ°æ•°æ®åº“
        5. æµ‹è¯•æœç´¢åŠŸèƒ½
    
    åŒ…å«çš„æ•°æ®æº:
        - MITRE ATT&CK æ”»å‡»æŠ€æœ¯åº“
        - OWASP Top 10 Webå®‰å…¨é£é™©
        - CWE Top 25 è½¯ä»¶å¼±ç‚¹
        - è‡ªå®šä¹‰URLåˆ—è¡¨ï¼ˆå‚å•†å…¬å‘Šã€PDFæŠ¥å‘Šç­‰ï¼‰
    """
    logging.info("=" * 60)
    logging.info("å¼€å§‹ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»ºPipeline")
    logging.info("=" * 60)
    
    # ========== ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºæ•°æ®åº“ ==========
    logging.info("\nã€æ­¥éª¤1ã€‘åˆ›å»ºå‘é‡æ•°æ®åº“")
    db_name, metric = create_database(metric_type=METRIC_TYPE)
    logging.info(f"âœ“ æ•°æ®åº“åˆ›å»ºæˆåŠŸ: {db_name} (metric={metric})")

    # ========== ç¬¬äºŒæ­¥ï¼šçˆ¬å–æ•°æ®æº ==========
    logging.info("\nã€æ­¥éª¤2ã€‘å¼€å§‹çˆ¬å–æ•°æ®æº")
    
    # 2.1 çˆ¬å– MITRE ATT&CK ï¼ˆé™åˆ¶50é¡µï¼Œé¿å…è¿‡åº¦è¯·æ±‚ï¼‰
    logging.info("\n>>> æ•°æ®æº1: MITRE ATT&CK æ”»å‡»æŠ€æœ¯åº“")
    mitre_pages = crawl_mitre_attack(max_pages=30)
    logging.info(f"âœ“ æŠ“å–åˆ° {len(mitre_pages)} ä¸ªMITREé¡µé¢")

    # 2.2 çˆ¬å– OWASP Top 10
    logging.info("\n>>> æ•°æ®æº2: OWASP Top 10 Webå®‰å…¨é£é™©")
    owasp_pages = crawl_owasp_top10()
    logging.info(f"âœ“ æŠ“å–åˆ° {len(owasp_pages)} ä¸ªOWASPé¡µé¢")
    
    # 2.3 çˆ¬å– CWE Top 25
    logging.info("\n>>> æ•°æ®æº3: CWE Top 25 è½¯ä»¶å¼±ç‚¹")
    cwe_pages = crawl_cwe_top25()
    logging.info(f"âœ“ æŠ“å–åˆ° {len(cwe_pages)} ä¸ªCWEé¡µé¢")

    # 2.4 è‡ªå®šä¹‰URLåˆ—è¡¨ï¼ˆä½ å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®æºï¼‰
    logging.info("\n>>> æ•°æ®æº4: è‡ªå®šä¹‰URLåˆ—è¡¨")
    extra_urls = [
        # ===== æ¨èçš„ä¸­æ–‡å®‰å…¨èµ„è®¯ç«™ç‚¹ =====
        # æ³¨æ„ï¼šä»¥ä¸‹URLä»…ä¸ºç¤ºä¾‹ï¼Œå®é™…çˆ¬å–æ—¶è¯·æ£€æŸ¥ç½‘ç«™çš„robots.txtå¹¶éµå®ˆçˆ¬è™«è§„åˆ™
        
        # FreeBuf æŠ€æœ¯æ–‡ç« ç¤ºä¾‹ï¼ˆæ›¿æ¢ä¸ºå…·ä½“æ–‡ç« URLï¼‰
        # "https://www.freebuf.com/articles/web/123456.html",
        
        # å®‰å…¨å®¢æŠ€æœ¯æ–‡ç« ç¤ºä¾‹
        # "https://www.anquanke.com/post/id/123456",
        
        # ===== å‚å•†å®‰å…¨å…¬å‘Šç¤ºä¾‹ =====
        # Microsoft å®‰å…¨å…¬å‘Š
        # "https://msrc.microsoft.com/update-guide/vulnerability/CVE-2021-XXXXX",
        
        # ===== PDFæŠ¥å‘Šç¤ºä¾‹ =====
        # NIST ç½‘ç»œå®‰å…¨æ¡†æ¶ï¼ˆå¦‚æœå¯ä»¥ç›´æ¥è®¿é—®PDFï¼‰
        # "https://nvlpubs.nist.gov/nistpubs/CSWP/NIST.CSWP.04162018.pdf",
        
        # ===== å®é™…å¯ç”¨çš„å…¬å¼€èµ„æº =====
        # NIST è®¡ç®—æœºå®‰å…¨èµ„æºä¸­å¿ƒï¼ˆä¸»é¡µï¼Œå¯ä»¥æå–æ¦‚å¿µæ€§å†…å®¹ï¼‰
        "https://csrc.nist.gov/",
        
        # US-CERT ç½‘ç»œå®‰å…¨æç¤º
        "https://www.cisa.gov/news-events/cybersecurity-advisories",
    ]
    extra_pages = crawl_generic_urls(extra_urls)
    logging.info(f"âœ“ æŠ“å–åˆ° {len(extra_pages)} ä¸ªè‡ªå®šä¹‰é¡µé¢")

    # åˆå¹¶æ‰€æœ‰é¡µé¢
    all_pages = mitre_pages + owasp_pages + cwe_pages + extra_pages
    logging.info(f"\nâœ“ æ€»è®¡çˆ¬å– {len(all_pages)} ä¸ªé¡µé¢")

    # ========== ç¬¬ä¸‰æ­¥ï¼šæ–‡æœ¬åˆ†å‰²ä¸å…ƒæ•°æ®ç”Ÿæˆ ==========
    logging.info("\nã€æ­¥éª¤3ã€‘æ–‡æœ¬åˆ†å‰²ä¸å…ƒæ•°æ®ç”Ÿæˆ")
    upload_items = []
    
    # éå†æ¯ä¸ªçˆ¬å–çš„é¡µé¢
    for p in all_pages:
        text = p["text"]
        title = p["title"]
        url = p["url"]
        meta_base = p.get("meta", {})
        
        # å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunk
        # chunk_size_chars=1800: æ¯ä¸ªchunkçº¦1800å­—ç¬¦ï¼ˆè€ƒè™‘åˆ°embeddingæ¨¡å‹çš„tokené™åˆ¶ï¼‰
        # chunk_overlap_chars=200: ç›¸é‚»chunké‡å 200å­—ç¬¦ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
        chunks = split_text_into_chunks(text, chunk_size_chars=1800, chunk_overlap_chars=200)
        
        # ä¸ºæ¯ä¸ªchunkæ·»åŠ å…ƒæ•°æ®
        for i, ch in enumerate(chunks):
            # å¤åˆ¶åŸºç¡€å…ƒæ•°æ®ï¼ˆåŒ…å«CVEã€æŠ€æœ¯IDã€åˆ†ç±»ç­‰ï¼‰
            meta = dict(meta_base)  # shallow copyé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            
            # æ·»åŠ chunkç‰¹æœ‰çš„å…ƒæ•°æ®
            meta.update({
                "source_url": url,          # æ¥æºURL
                "source_title": title,      # æ¥æºæ ‡é¢˜
                "chunk_index": i,           # å½“å‰chunkåœ¨åŸæ–‡ä¸­çš„ä½ç½®
                "chunk_length": len(ch),    # chunkçš„å­—ç¬¦æ•°
                "total_chunks": len(chunks) # è¯¥æ–‡æ¡£æ€»chunkæ•°
            })
            
            # æ„é€ ä¸Šä¼ æ ¼å¼ï¼š{'file': æ–‡æœ¬å†…å®¹, 'metadata': å…ƒæ•°æ®}
            upload_items.append({"file": ch, "metadata": meta})
    
    logging.info(f"âœ“ ç”Ÿæˆ {len(upload_items)} ä¸ªæ–‡æœ¬chunkï¼ˆå¹³å‡æ¯é¡µ {len(upload_items)/len(all_pages):.1f} ä¸ªchunkï¼‰")

    # ========== ç¬¬å››æ­¥ï¼šæ‰¹é‡ä¸Šä¼ åˆ°æ•°æ®åº“ ==========
    logging.info("\nã€æ­¥éª¤4ã€‘æ‰¹é‡ä¸Šä¼ åˆ°å‘é‡æ•°æ®åº“")
    file_ids = upload_chunks(db_name, upload_items)
    logging.info(f"âœ“ ä¸Šä¼ å®Œæˆï¼Œå…± {len(file_ids)} ä¸ªæ–‡æœ¬å—")

    # ========== ç¬¬äº”æ­¥ï¼šæµ‹è¯•æœç´¢åŠŸèƒ½ ==========
    logging.info("\nã€æ­¥éª¤5ã€‘æµ‹è¯•æœç´¢åŠŸèƒ½")
    
    # æµ‹è¯•æŸ¥è¯¢1ï¼šé’“é±¼é‚®ä»¶ç‰¹å¾
    test_queries = [
        "phishing mail indicators",           # é’“é±¼é‚®ä»¶æŒ‡æ ‡
        "SQLæ³¨å…¥æ”»å‡»åŸç†",                      # SQLæ³¨å…¥
        "å¦‚ä½•é˜²å¾¡è·¨ç«™è„šæœ¬æ”»å‡»",                 # XSSé˜²å¾¡
        "è¿œç¨‹ä»£ç æ‰§è¡Œæ¼æ´",                     # RCEæ¼æ´
    ]
    
    for query in test_queries[:2]:  # åªæµ‹è¯•å‰2ä¸ªæŸ¥è¯¢
        logging.info(f"\næµ‹è¯•æŸ¥è¯¢: '{query}'")
        payload = {
            "token": TOKEN, 
            "query": query, 
            "top_k": 3,              # è¿”å›å‰3ä¸ªæœ€ç›¸å…³ç»“æœ
            "metric_type": metric
        }
        
        try:
            resp = requests.post(f"{BASE_URL}/databases/{db_name}/search", json=payload)
            resp.raise_for_status()
            results = resp.json()
            
            # æ˜¾ç¤ºæœç´¢ç»“æœ
            if "results" in results and results["results"]:
                for idx, item in enumerate(results["results"][:3], 1):
                    logging.info(f"  [{idx}] ç›¸ä¼¼åº¦: {item.get('distance', 'N/A'):.4f}")
                    logging.info(f"      æ ‡é¢˜: {item.get('metadata', {}).get('source_title', 'N/A')[:50]}")
                    logging.info(f"      åˆ†ç±»: {item.get('metadata', {}).get('categories', [])}")
            else:
                logging.info("  æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        except Exception as e:
            logging.error(f"  æœç´¢å¤±è´¥: {e}")
    
    logging.info("\n" + "=" * 60)
    logging.info("Pipelineæ‰§è¡Œå®Œæˆï¼")
    logging.info(f"æ•°æ®åº“åç§°: {db_name}")
    logging.info(f"æ€»æ–‡æ¡£æ•°: {len(all_pages)}")
    logging.info(f"æ€»chunkæ•°: {len(file_ids)}")
    logging.info("=" * 60)

    return db_name, file_ids

# ==================== ç¨‹åºå…¥å£ ====================

if __name__ == "__main__":
    """
    ç¨‹åºä¸»å…¥å£
    
    è¿è¡Œæ–¹å¼:
        python createdb_pipeline.py
    
    è¿è¡Œå‰å‡†å¤‡:
        1. ç¡®ä¿å·²å®‰è£…ä¾èµ–: pip install requests beautifulsoup4 pdfminer.six
        2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œç¡®ä¿å¯ä»¥è®¿é—®ç›®æ ‡ç½‘ç«™
        3. ä¿®æ”¹é…ç½®åŒºçš„ TOKENã€USER_NAMEã€BASE_URL
        4. æ ¹æ®éœ€æ±‚è°ƒæ•´çˆ¬å–æ•°é‡å’Œæ•°æ®æº
    
    é¢„æœŸç»“æœ:
        - åˆ›å»ºä¸€ä¸ªæ–°çš„å‘é‡æ•°æ®åº“
        - çˆ¬å–å¹¶ä¸Šä¼ ç½‘ç»œå®‰å…¨çŸ¥è¯†
        - è¾“å‡ºæ•°æ®åº“åç§°å’Œç»Ÿè®¡ä¿¡æ¯
    """
    try:
        logging.info("\n")
        logging.info("*" * 60)
        logging.info("     ç½‘ç»œå®‰å…¨çŸ¥è¯†åº“æ„å»º Pipeline")
        logging.info("*" * 60)
        logging.info(f"é…ç½®: BASE_URL={BASE_URL}")
        logging.info(f"é…ç½®: USER_NAME={USER_NAME}")
        logging.info(f"é…ç½®: METRIC_TYPE={METRIC_TYPE}")
        logging.info("*" * 60)
        
        # æ‰§è¡Œä¸»æµç¨‹
        db, fids = pipeline_demo()
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        logging.info("\n")
        logging.info("ğŸ‰ Pipelineæ‰§è¡ŒæˆåŠŸï¼")
        logging.info(f"ğŸ“Š æ•°æ®åº“åç§°: {db}")
        logging.info(f"ğŸ“Š ä¸Šä¼ æ–‡ä»¶æ•°: {len(fids)}")
        logging.info(f"ğŸ’¡ åç»­å¯ä»¥ä½¿ç”¨æ­¤æ•°æ®åº“è¿›è¡ŒRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ")
        logging.info(f"ğŸ’¡ åœ¨ä½ çš„åº”ç”¨ä¸­ä½¿ç”¨æ•°æ®åº“å: {db}")
        
    except KeyboardInterrupt:
        logging.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logging.error(f"\nâŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise
